{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'cleaning_utils' from 'c:\\\\Users\\\\34642\\\\OneDrive\\\\Escritorio\\\\Synthesis Project I\\\\Notebooks\\\\../utils\\\\cleaning_utils.py'>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import sys\n",
    "import random\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from matplotlib import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from IPython.display import display\n",
    "import importlib\n",
    "\n",
    "sys.path.append('../utils') \n",
    "import cleaning_utils as cu\n",
    "\n",
    "importlib.reload(cu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environmental Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "vP_0tHxQo39T"
   },
   "outputs": [],
   "source": [
    "data_path = os.path.join(\"..\", \"Data\")\n",
    "schedules_df = pd.read_excel(os.path.join(data_path, \"Schedules.xlsx\"))\n",
    "data_df = pd.read_excel(os.path.join(data_path, \"Data.xlsx\"))\n",
    "clients_df = pd.read_excel(os.path.join(data_path, \"Clients.xlsx\"))\n",
    "transl_cost_pairs_df = pd.read_excel(os.path.join(data_path, \"TranslatorsCost+Pairs.xlsx\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df['START'] = pd.to_datetime(data_df['START'], errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Envirinmental Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "\n",
    "TRANSLATORS_UNAVAILABLE = []\n",
    "\n",
    "wildcards = [None, \"Quality\", \"Time\", \"Cost\"]\n",
    "task_types = data_df[\"TASK_TYPE\"].unique()\n",
    "unique_language_pairs = data_df[[\"SOURCE_LANG\", \"TARGET_LANG\"]].drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note:** `TRANSLATORS_UNAVAILABLE` is a list for keeping track of translators alredy assigned or performing a task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO we have to think what to do with translators that can perform multiple tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping invalid rows in column 'START'...\n",
      "No invalid dates found in column 'END'.\n",
      "No invalid dates found in column 'DELIVERED'.\n",
      "No invalid dates found in column 'ASSIGNED'.\n",
      " \n",
      "Invalid START dates:\n",
      " 91127   NaT\n",
      "Name: START, dtype: datetime64[ns] \n",
      "\n",
      "Invalid END dates:\n",
      " Series([], Name: END, dtype: datetime64[ns]) \n",
      "\n",
      "Invalid DELIVERED dates:\n",
      " Series([], Name: ASSIGNED, dtype: datetime64[ns]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Function to detect invalid rows\n",
    "data_df, start_invalid_dates = cu.drop_invalid_dates(data_df, 'START')\n",
    "data_df, end_invalid_dates = cu.drop_invalid_dates(data_df, 'END')\n",
    "data_df, delivered_invalid_dates = cu.drop_invalid_dates(data_df, 'DELIVERED')\n",
    "data_df, delivered_invalid_dates = cu.drop_invalid_dates(data_df, 'ASSIGNED')\n",
    "\n",
    "print(\" \")\n",
    "\n",
    "# Show the invalid dates in each column\n",
    "print(\"Invalid START dates:\\n\", start_invalid_dates, \"\\n\")\n",
    "print(\"Invalid END dates:\\n\", end_invalid_dates, \"\\n\")\n",
    "print(\"Invalid DELIVERED dates:\\n\", delivered_invalid_dates, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 25764 rows with missing values or START >= END.\n"
     ]
    }
   ],
   "source": [
    "data_df = cu.drop_invalid_rows(data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task Information\n",
    "##### General Info\n",
    "- **PROJECT_ID**: Project code (additional info, likely not necessary).\n",
    "- **PM**: Responsible management team.\n",
    "- **TASK_ID**: Task code.\n",
    "##### Dates\n",
    "- **START**: Task start date.\n",
    "- **END**: Theoretical task delivery date (can be compared with `DELIVERED` to check for delays).\n",
    "##### Task Type (`TASK_TYPE`)\n",
    "Some considerations must be taken into account:\n",
    "- **DTP**: Desktop-Publishing tasks.\n",
    "- **Engineering**: Engineering tasks such as file conversions, coding, etc.\n",
    "- **LanguageLead**: Linguistic management tasks. Assigned to highly experienced and quality-oriented individuals who regularly work on the project.\n",
    "- **Management**: General management tasks.\n",
    "- **Miscellaneous**: Various linguistic tasks.\n",
    "- **PostEditing**: Post-editing tasks. Similar to Translation tasks but with slightly different skills required for the TRANSLATOR.\n",
    "- **ProofReading**: Full review of a Translation or PostEditing. Always follows a Translation or PostEditing. The TRANSLATOR assigned must have more \n",
    "experience than the person who performed the initial step.\n",
    "- **Spotcheck**: Partial review of a Translation or PostEditing. Similar conditions as ProofReading.\n",
    "- **TEST**: Test required to qualify for working with a client. Should be assigned to the most experienced and high-quality TRANSLATOR \n",
    "for the client or topic, regardless of price but considering the deadline.\n",
    "- **Training**: Translator experience and quality are not considered.\n",
    "- **Translation**: Translation task. The translator’s quality can be slightly lower if the ProofReading (not Spotcheck) is done by a superior. If \n",
    "Spotcheck is done, the required quality must be met.\n",
    "##### Languages\n",
    "- **SOURCE_LANG**: Source language.\n",
    "- **TARGET_LANG**: Target language.\n",
    "##### Workflow \n",
    "- **TRANSLATOR**: Translator responsible for the task.\n",
    "- **ASSIGNED**: Time when the task is assigned (pre-notice) to the TRANSLATOR.\n",
    "- **READY**: Time when the TRANSLATOR is notified they can start.\n",
    "- **WORKING**: Time when the TRANSLATOR starts the task.\n",
    "- **DELIVERED**: Time when the TRANSLATOR delivers the task.\n",
    "- **RECEIVED**: Time when the PM receives the task.\n",
    "- **CLOSE**: Time when the PM marks the task as completed.\n",
    "##### Cost & Quality\n",
    "- **FORECAST**: Estimated hours for completion.\n",
    "- **HOURLY_RATE**: Task hourly rate.\n",
    "- **COST**: Total task cost.\n",
    "- **QUALITY_EVALUATION**: Quality control evaluation.\n",
    "##### Client Info\n",
    "- **MANUFACTURER**: Client.\n",
    "- **MANUFACTURER_SECTOR**: Level 1 client categorization.\n",
    "- **MANUFACTURER_INDUSTRY_GROUP**: Level 2 client categorization.\n",
    "- **MANUFACTURER_INDUSTRY**: Level 3 client categorization.\n",
    "- **MANUFACTURER_SUBINDUSTRY**: Level 4 client categorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Task:\n",
    "    def __init__(self, **kwargs):\n",
    "        \"\"\"\n",
    "        A class used to represent a Task. It initializes the attributes dynamically \n",
    "        using the keyword arguments passed. Default values are provided for certain fields.\n",
    "        \"\"\"\n",
    "        self.PROJECT_ID = kwargs.get('PROJECT_ID', None)\n",
    "        self.TASK_ID = kwargs.get('TASK_ID', None)\n",
    "        self.ASSIGNED = kwargs.get('ASSIGNED', None)\n",
    "        self.END = kwargs.get('END', None)\n",
    "        self.SELLING_HOURLY_PRICE = kwargs.get('SELLING_HOURLY_PRICE', None)\n",
    "        self.MIN_QUALITY = kwargs.get('MIN_QUALITY', None)\n",
    "        self.WILDCARD = kwargs.get('WILDCARD', None) \n",
    "        self.TASK_TYPE = kwargs.get('TASK_TYPE', None)\n",
    "        self.SOURCE_LANG = kwargs.get('SOURCE_LANG', None)\n",
    "        self.TARGET_LANG = kwargs.get('TARGET_LANG', None)\n",
    "        self.MANUFACTURER = kwargs.get('MANUFACTURER', None)\n",
    "        self.MANUFACTURER_SECTOR = kwargs.get('MANUFACTURER_SECTOR', None)\n",
    "        self.MANUFACTURER_INDUSTRY_GROUP = kwargs.get('MANUFACTURER_INDUSTRY_GROUP', None)\n",
    "        self.MANUFACTURER_INDUSTRY = kwargs.get('MANUFACTURER_INDUSTRY', None)\n",
    "        self.MANUFACTURER_SUBINDUSTRY = kwargs.get('MANUFACTURER_SUBINDUSTRY', None)\n",
    "        \n",
    "        # Optional attributes with None default value\n",
    "        self.START = kwargs.get('START', None)\n",
    "        self.PM = kwargs.get('PM', None)\n",
    "        self.TRANSLATOR = kwargs.get('TRANSLATOR', None)\n",
    "        self.READY = kwargs.get('READY', None)\n",
    "        self.WORKING = kwargs.get('WORKING', None)\n",
    "        self.DELIVERED = kwargs.get('DELIVERED', None)\n",
    "        self.RECEIVED = kwargs.get('RECEIVED', None)\n",
    "        self.CLOSE = kwargs.get('CLOSE', None)\n",
    "        self.FORECAST = kwargs.get('FORECAST', None)\n",
    "        self.HOURLY_RATE = kwargs.get('HOURLY_RATE', None)\n",
    "        self.COST = kwargs.get('COST', None)\n",
    "        self.QUALITY_EVALUATION = kwargs.get('QUALITY_EVALUATION', None)\n",
    "        \n",
    "    \n",
    "    def __str__(self):\n",
    "        return (\n",
    "            f\"Task Details:\\n\"\n",
    "            f\"  - Task ID: {self.TASK_ID}\\n\"\n",
    "            f\"  - Type: {self.TASK_TYPE}\\n\"\n",
    "            f\"  - Client: {self.MANUFACTURER}\\n\"\n",
    "            f\"  - Sector: {self.MANUFACTURER_SECTOR}\\n\"\n",
    "            f\"  - Industry (Subsector): {self.MANUFACTURER_INDUSTRY}\\n\"\n",
    "            f\"  - Start: {self.START}\\n\"\n",
    "            f\"  - Budget: {self.SELLING_HOURLY_PRICE}\\n\"\n",
    "            f\"  - Quality: {self.MIN_QUALITY}\\n\"\n",
    "            f\"  - Wildcard: {self.WILDCARD}\\n\"\n",
    "            f\"  - Source Language: {self.SOURCE_LANG}\\n\"\n",
    "            f\"  - Target Language: {self.TARGET_LANG}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and validation (e.g., 80% train, 20% validation)\n",
    "train_df, validation_df = train_test_split(data_df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_and_save_translator_labels(df, translator_column=\"TRANSLATOR\"):\n",
    "    \"\"\"\n",
    "    Extracts and removes translators from test_df, then saves them as a label dict.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The dataframe containing translator data.\n",
    "        translator_column (str): The column name that holds the translator labels.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: The test_df without the translators column.\n",
    "        dict: Dictionary of translator labels {index: translators}\n",
    "    \"\"\"\n",
    "    if translator_column not in df.columns:\n",
    "        raise ValueError(f\"Column '{translator_column}' not found in test_df.\")\n",
    "    \n",
    "    # Extract labels\n",
    "    translator_labels = df[translator_column].to_dict()\n",
    "    \n",
    "    # Drop the column from the DataFrame\n",
    "    df = df.drop(columns=[translator_column])\n",
    "    \n",
    "    return df, translator_labels\n",
    "\n",
    "train_df_clean, train_translator_labels = drop_and_save_translator_labels(train_df)\n",
    "validation_df_clean, validation_translator_labels = drop_and_save_translator_labels(validation_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PROJECT_ID</th>\n",
       "      <th>PM</th>\n",
       "      <th>TASK_ID</th>\n",
       "      <th>START</th>\n",
       "      <th>END</th>\n",
       "      <th>TASK_TYPE</th>\n",
       "      <th>SOURCE_LANG</th>\n",
       "      <th>TARGET_LANG</th>\n",
       "      <th>ASSIGNED</th>\n",
       "      <th>READY</th>\n",
       "      <th>WORKING</th>\n",
       "      <th>DELIVERED</th>\n",
       "      <th>RECEIVED</th>\n",
       "      <th>CLOSE</th>\n",
       "      <th>FORECAST</th>\n",
       "      <th>HOURLY_RATE</th>\n",
       "      <th>COST</th>\n",
       "      <th>QUALITY_EVALUATION</th>\n",
       "      <th>MANUFACTURER</th>\n",
       "      <th>MANUFACTURER_SECTOR</th>\n",
       "      <th>MANUFACTURER_INDUSTRY_GROUP</th>\n",
       "      <th>MANUFACTURER_INDUSTRY</th>\n",
       "      <th>MANUFACTURER_SUBINDUSTRY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>106193</th>\n",
       "      <td>212576</td>\n",
       "      <td>PMT</td>\n",
       "      <td>10313113</td>\n",
       "      <td>2014-11-06 15:29:00</td>\n",
       "      <td>2014-11-10 09:00:00</td>\n",
       "      <td>Translation</td>\n",
       "      <td>English</td>\n",
       "      <td>Spanish (LA)</td>\n",
       "      <td>2014-11-06 15:39:16</td>\n",
       "      <td>2014-11-06 15:39:18</td>\n",
       "      <td>2014-11-06 23:18:08</td>\n",
       "      <td>2014-11-10 00:00:44</td>\n",
       "      <td>2014-11-10 12:17:09</td>\n",
       "      <td>2014-11-10 12:17:09</td>\n",
       "      <td>5.78</td>\n",
       "      <td>13</td>\n",
       "      <td>75.14</td>\n",
       "      <td>5</td>\n",
       "      <td>Sparklight Media</td>\n",
       "      <td>Communication Services</td>\n",
       "      <td>Media</td>\n",
       "      <td>Interactive Media &amp; Services</td>\n",
       "      <td>Interactive Media &amp; Services</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153803</th>\n",
       "      <td>214379</td>\n",
       "      <td>RMT</td>\n",
       "      <td>10419128</td>\n",
       "      <td>2016-01-25 16:00:00</td>\n",
       "      <td>2016-01-26 12:00:00</td>\n",
       "      <td>ProofReading</td>\n",
       "      <td>English</td>\n",
       "      <td>Catalan</td>\n",
       "      <td>2016-01-25 12:12:58</td>\n",
       "      <td>2016-01-25 12:57:40</td>\n",
       "      <td>2016-01-25 15:35:55</td>\n",
       "      <td>2016-01-25 16:12:16</td>\n",
       "      <td>2016-01-25 16:17:35</td>\n",
       "      <td>2016-01-25 16:17:35</td>\n",
       "      <td>0.29</td>\n",
       "      <td>15</td>\n",
       "      <td>4.35</td>\n",
       "      <td>5</td>\n",
       "      <td>TrueConnect</td>\n",
       "      <td>Communication Services</td>\n",
       "      <td>Interactive Media &amp; Services</td>\n",
       "      <td>Internet Services &amp; Infrastructure</td>\n",
       "      <td>Internet Services &amp; Infrastructure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283706</th>\n",
       "      <td>217532</td>\n",
       "      <td>KMT</td>\n",
       "      <td>10694449</td>\n",
       "      <td>2019-01-07 18:50:00</td>\n",
       "      <td>2019-01-07 21:00:00</td>\n",
       "      <td>Translation</td>\n",
       "      <td>English</td>\n",
       "      <td>Portuguese (Brazil)</td>\n",
       "      <td>2019-01-07 19:12:29</td>\n",
       "      <td>2019-01-07 19:12:57</td>\n",
       "      <td>2019-01-07 19:14:00</td>\n",
       "      <td>2019-01-07 19:16:03</td>\n",
       "      <td>2019-01-07 21:07:58</td>\n",
       "      <td>2019-01-07 21:08:04</td>\n",
       "      <td>0.03</td>\n",
       "      <td>15</td>\n",
       "      <td>0.45</td>\n",
       "      <td>10</td>\n",
       "      <td>SoftEcology</td>\n",
       "      <td>Information Technology</td>\n",
       "      <td>Software</td>\n",
       "      <td>Application Software</td>\n",
       "      <td>Environmental Software</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493944</th>\n",
       "      <td>221084</td>\n",
       "      <td>KMT</td>\n",
       "      <td>11109570</td>\n",
       "      <td>2022-02-24 15:58:00</td>\n",
       "      <td>2022-02-24 16:55:00</td>\n",
       "      <td>ProofReading</td>\n",
       "      <td>English</td>\n",
       "      <td>Spanish (LA)</td>\n",
       "      <td>2022-02-24 15:20:31</td>\n",
       "      <td>2022-02-24 15:57:26</td>\n",
       "      <td>2022-02-24 16:08:34</td>\n",
       "      <td>2022-02-24 16:20:36</td>\n",
       "      <td>2022-02-24 16:49:31</td>\n",
       "      <td>2022-02-24 16:55:10</td>\n",
       "      <td>0.00</td>\n",
       "      <td>14</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6</td>\n",
       "      <td>HealthyLife</td>\n",
       "      <td>Health Care</td>\n",
       "      <td>Health Care Providers</td>\n",
       "      <td>Health Care Facilities</td>\n",
       "      <td>Long-Term Care Facilities</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171666</th>\n",
       "      <td>214396</td>\n",
       "      <td>PMT</td>\n",
       "      <td>10455761</td>\n",
       "      <td>2016-07-04 15:34:00</td>\n",
       "      <td>2016-07-06 11:00:00</td>\n",
       "      <td>Translation</td>\n",
       "      <td>English</td>\n",
       "      <td>Spanish (Iberian)</td>\n",
       "      <td>2016-07-04 15:41:18</td>\n",
       "      <td>2016-07-04 15:43:01</td>\n",
       "      <td>2016-07-05 09:44:51</td>\n",
       "      <td>2016-07-06 11:04:49</td>\n",
       "      <td>2016-07-08 09:50:06</td>\n",
       "      <td>2016-07-08 09:50:06</td>\n",
       "      <td>4.33</td>\n",
       "      <td>16</td>\n",
       "      <td>69.28</td>\n",
       "      <td>5</td>\n",
       "      <td>WoodWorks</td>\n",
       "      <td>Consumer Discretionary</td>\n",
       "      <td>Leisure Products</td>\n",
       "      <td>Leisure Products</td>\n",
       "      <td>Leisure Products</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       PROJECT_ID   PM   TASK_ID               START                 END  \\\n",
       "106193     212576  PMT  10313113 2014-11-06 15:29:00 2014-11-10 09:00:00   \n",
       "153803     214379  RMT  10419128 2016-01-25 16:00:00 2016-01-26 12:00:00   \n",
       "283706     217532  KMT  10694449 2019-01-07 18:50:00 2019-01-07 21:00:00   \n",
       "493944     221084  KMT  11109570 2022-02-24 15:58:00 2022-02-24 16:55:00   \n",
       "171666     214396  PMT  10455761 2016-07-04 15:34:00 2016-07-06 11:00:00   \n",
       "\n",
       "           TASK_TYPE SOURCE_LANG          TARGET_LANG            ASSIGNED  \\\n",
       "106193   Translation     English         Spanish (LA) 2014-11-06 15:39:16   \n",
       "153803  ProofReading     English              Catalan 2016-01-25 12:12:58   \n",
       "283706   Translation     English  Portuguese (Brazil) 2019-01-07 19:12:29   \n",
       "493944  ProofReading     English         Spanish (LA) 2022-02-24 15:20:31   \n",
       "171666   Translation     English    Spanish (Iberian) 2016-07-04 15:41:18   \n",
       "\n",
       "                     READY             WORKING           DELIVERED  \\\n",
       "106193 2014-11-06 15:39:18 2014-11-06 23:18:08 2014-11-10 00:00:44   \n",
       "153803 2016-01-25 12:57:40 2016-01-25 15:35:55 2016-01-25 16:12:16   \n",
       "283706 2019-01-07 19:12:57 2019-01-07 19:14:00 2019-01-07 19:16:03   \n",
       "493944 2022-02-24 15:57:26 2022-02-24 16:08:34 2022-02-24 16:20:36   \n",
       "171666 2016-07-04 15:43:01 2016-07-05 09:44:51 2016-07-06 11:04:49   \n",
       "\n",
       "                  RECEIVED               CLOSE  FORECAST  HOURLY_RATE   COST  \\\n",
       "106193 2014-11-10 12:17:09 2014-11-10 12:17:09      5.78           13  75.14   \n",
       "153803 2016-01-25 16:17:35 2016-01-25 16:17:35      0.29           15   4.35   \n",
       "283706 2019-01-07 21:07:58 2019-01-07 21:08:04      0.03           15   0.45   \n",
       "493944 2022-02-24 16:49:31 2022-02-24 16:55:10      0.00           14   0.00   \n",
       "171666 2016-07-08 09:50:06 2016-07-08 09:50:06      4.33           16  69.28   \n",
       "\n",
       "        QUALITY_EVALUATION      MANUFACTURER     MANUFACTURER_SECTOR  \\\n",
       "106193                   5  Sparklight Media  Communication Services   \n",
       "153803                   5       TrueConnect  Communication Services   \n",
       "283706                  10       SoftEcology  Information Technology   \n",
       "493944                   6       HealthyLife             Health Care   \n",
       "171666                   5         WoodWorks  Consumer Discretionary   \n",
       "\n",
       "         MANUFACTURER_INDUSTRY_GROUP               MANUFACTURER_INDUSTRY  \\\n",
       "106193                         Media        Interactive Media & Services   \n",
       "153803  Interactive Media & Services  Internet Services & Infrastructure   \n",
       "283706                      Software                Application Software   \n",
       "493944         Health Care Providers              Health Care Facilities   \n",
       "171666              Leisure Products                    Leisure Products   \n",
       "\n",
       "                  MANUFACTURER_SUBINDUSTRY  \n",
       "106193        Interactive Media & Services  \n",
       "153803  Internet Services & Infrastructure  \n",
       "283706              Environmental Software  \n",
       "493944           Long-Term Care Facilities  \n",
       "171666                    Leisure Products  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task Index: 89970, Translator: Isaias Venancio\n",
      "Task Index: 132154, Translator: Xoana\n",
      "Task Index: 544858, Translator: Mariano Fidel\n",
      "Task Index: 36852, Translator: Lucia\n",
      "Task Index: 280522, Translator: Victorino Salvio\n",
      "Task Index: 59640, Translator: Laurina Rafael\n",
      "Task Index: 133792, Translator: Oscar\n",
      "Task Index: 44845, Translator: Andres\n",
      "Task Index: 252248, Translator: Laurina Rafael\n",
      "Task Index: 111992, Translator: Dimas Rafael\n",
      "Task Index: 129220, Translator: Hildegarda Leonor\n"
     ]
    }
   ],
   "source": [
    "for i, (key, value) in enumerate(validation_translator_labels.items()):\n",
    "    print(f\"Task Index: {key}, Translator: {value}\")\n",
    "    if i == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation of what is happening\n",
    "\n",
    "In the global scope:\n",
    "\n",
    "0. We use as base of useful features the dataframe `transl_cost_pairs_df`\n",
    "It includes:\n",
    "- TRANSLATOR: Translator name.\n",
    "- SOURCE_LANG: Source language.\n",
    "- TARGET_LANG: Target language.\n",
    "- HOURLY_RATE: Cost per hour.\n",
    "\n",
    "\n",
    "1. We compute the **average proportional delay** (speed) from the `data_df` for each translator and merge it with this base dataframe\n",
    "\n",
    "\n",
    "Now for the each task:\n",
    "\n",
    "2. We start to the strict filter:\n",
    "    - Filter of **languages**: only consider the translators who offer this translation\n",
    "    - Filter of **price**: only consider the prices below the threshold\n",
    "    - Filter of **quality** by language: this is done by making an average of the quality of these languages for each translator, and then using it as a threshold. \n",
    "    - Filter of **availability**: if the taks lasts less than 7 days we check whether the translator will even work before the theoretical deadline (it is 7 days because everyone works at least once a week)\n",
    "\n",
    "3. We do a **weighted knn**:\n",
    "    - We do it on the *perfect point* (price = 0, quality = 10, speed = 100%, experience = 10... (orientative values))\n",
    "    - The weights are chosen by the wildcards and by the expereince required for the type of task. A weighted knn, after normalizing, it distorts the chosen axis size to give more or less weight. \n",
    "    - The similarity score is the final ranking. \n",
    "\n",
    "4. Outcome possibilities: \n",
    "    - We get None or too few translators: if it is None we use the wildcard to completely ignore that factor in the strict filter\n",
    "    - We get a lot of translators: (we need the rest of features to give better recommendations)\n",
    "\n",
    "5. Calculate accuracy and find a way to retrieve explainations\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- THESE ARE GENERAL FUNCTIONS THAT CAN BE USED TO ACTUALIZE THE DATA IF NEW TASKS WERE ADDED TO THE DATASET -----\n",
    "def compute_delay_percentage(data_df, transl_cost_pairs_df):\n",
    "    \"\"\"\n",
    "    Compute the delay percentage of each translator based on task completion times.\n",
    "    Negative values indicate early delivery, positive values indicate late delivery.\n",
    "\n",
    "    Args:\n",
    "        data_df (pd.DataFrame): Task data with 'TRANSLATOR', 'START', 'END', 'DELIVERED'\n",
    "        transl_cost_pairs_df (pd.DataFrame): Translator costs with 'TRANSLATOR', 'COST'\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Merged dataframe with 'TRANSLATOR', 'COST', 'AVG_DELAY_PERCENTAGE'\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Convert date columns to datetime safely\n",
    "        date_cols = ['START', 'END', 'DELIVERED']\n",
    "        for col in date_cols:\n",
    "            data_df[col] = pd.to_datetime(data_df[col], errors='coerce')\n",
    "\n",
    "        # Calculate duration and filter out bad rows\n",
    "        duration = data_df['END'] - data_df['START']\n",
    "        invalid_rows = duration <= pd.Timedelta(0)\n",
    "        if invalid_rows.any():\n",
    "            print(f\"[INFO] Removed {invalid_rows.sum()} rows with zero or negative durations.\")\n",
    "            data_df = data_df[~invalid_rows]\n",
    "            duration = duration[~invalid_rows]\n",
    "\n",
    "        # Calculate delay percentage\n",
    "        delay = (data_df['DELIVERED'] - data_df['END']) / duration * 100\n",
    "        delay = delay.replace([np.inf, -np.inf, np.nan], 0).clip(-100, 100)\n",
    "        data_df = data_df.copy()\n",
    "        data_df['DELAY_PERCENTAGE'] = delay\n",
    "\n",
    "        # Compute average delay per translator\n",
    "        avg_delay = (\n",
    "            data_df.groupby('TRANSLATOR')['DELAY_PERCENTAGE']\n",
    "            .mean()\n",
    "            .round(2)\n",
    "            .reset_index()\n",
    "            .rename(columns={'DELAY_PERCENTAGE': 'AVG_DELAY_PERCENTAGE'})\n",
    "        )\n",
    "\n",
    "        # Merge with cost data\n",
    "        merged = transl_cost_pairs_df.merge(avg_delay, on='TRANSLATOR', how='left')\n",
    "        merged['AVG_DELAY_PERCENTAGE'] = merged['AVG_DELAY_PERCENTAGE'].fillna(0)\n",
    "\n",
    "        return merged\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed to compute delay percentages: {e}\")\n",
    "        return transl_cost_pairs_df.assign(AVG_DELAY_PERCENTAGE=0)\n",
    "\n",
    "\n",
    "def compute_number_tasks(data_df, translators_attributes_df):\n",
    "    \"\"\"\n",
    "    Computes the number of tasks for each translator.\n",
    "    \n",
    "    Args:\n",
    "        data_df (pd.DataFrame): \n",
    "            DataFrame containing the data of the tasks.\n",
    "        df_filtered (pd.DataFrame): \n",
    "            DataFrame containing the filtered translators' attributes.\n",
    "\n",
    "    Returns:\n",
    "        translators_attributes_df (pd.DataFrame) with the delay_percentage.\n",
    "    \"\"\"\n",
    "    #TAKE INTO ACCOUNT: Task count is added to the dataset to help judge reliability (e.g if the quality of translator is calculated using a few tasks). \n",
    "\n",
    "    # Count the number of tasks each translator has done\n",
    "    task_counts = data_df.groupby('TRANSLATOR').size().reset_index(name='NUM_TASKS')\n",
    "\n",
    "    # Merge the task counts into the filtered dataframe\n",
    "    translators_attributes_df = translators_attributes_df.merge(task_counts, on='TRANSLATOR', how='left')\n",
    "\n",
    "    # Fill missing values (i.e., translators with no tasks) with 0\n",
    "    translators_attributes_df['NUM_TASKS'] = translators_attributes_df['NUM_TASKS'].fillna(0).astype(int)\n",
    "\n",
    "    return translators_attributes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1742052911892,
     "user": {
      "displayName": "Rime Slaoui",
      "userId": "06907206809915164567"
     },
     "user_tz": -60
    },
    "id": "DdpXWG2XyfuP",
    "outputId": "704065d5-17d5-4fe8-fd7c-f45157aa07f6"
   },
   "outputs": [],
   "source": [
    "# ----- THESE ARE FUNCTIONS TO CALCULATE QUALITY AND EXPERIENCE (ONCE FILTERED BY LANGUAGES AND PRICE) -----\n",
    "def compute_quality_by_languages(data_df, df_filtered, source_lang, target_lang):\n",
    "    \"\"\"\n",
    "    Computes average quality for a given language pair (source_lang → target_lang).\n",
    "    If the translator has no experience with that task, falls back to:\n",
    "      - their overall average quality (with a penalty), or\n",
    "      - a 5 if no task has been done.\n",
    "    \n",
    "    Args:\n",
    "        df_filtered (pd.DataFrame): Filtered translators.\n",
    "        source_lang (str): Source language.\n",
    "        target_lang (str): Target language.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Same df_filtered with new 'AVG_QUALITY_BY_LG' column.\n",
    "    \"\"\"\n",
    "    if df_filtered.empty:\n",
    "        print(f\"Warning: No translators found in the filtered dataframe.\")\n",
    "        return df_filtered\n",
    "    \n",
    "    translators = df_filtered['TRANSLATOR'].unique()\n",
    "    \n",
    "    # Filter tasks dataframe by the language pair and translators in df_filtered\n",
    "    mask_lang_pair = (\n",
    "        (data_df['SOURCE_LANG'] == source_lang) &\n",
    "        (data_df['TARGET_LANG'] == target_lang) &\n",
    "        (data_df['TRANSLATOR'].isin(df_filtered['TRANSLATOR']))\n",
    "    )\n",
    "\n",
    "\n",
    "    # Compute the average quality for each translator in the filtered dataframe\n",
    "    avg_quality = (\n",
    "        data_df[mask_lang_pair]\n",
    "        .groupby('TRANSLATOR')['QUALITY_EVALUATION']\n",
    "        .mean()\n",
    "        .round(2)\n",
    "    )\n",
    "\n",
    "    # Assing the average quality to the filtered df\n",
    "    df_filtered['AVG_QUALITY_BY_LG'] = df_filtered['TRANSLATOR'].map(avg_quality)\n",
    "\n",
    "    # Fallback to penalized overall average\n",
    "    mask_missing = df_filtered['AVG_QUALITY_BY_LG'].isna()\n",
    "\n",
    "    overall_avg = (\n",
    "        data_df[data_df['TRANSLATOR'].isin(translators)]\n",
    "        .groupby('TRANSLATOR')['QUALITY_EVALUATION']\n",
    "        .mean()\n",
    "        .round(2)\n",
    "        .apply(lambda x: x - 1 if pd.notnull(x) else None)  # configurable penalization, for flexibility\n",
    "        #For a data-driven approach, use can standard deviation or percentile-based penalization to adapt to the distribution of quality scores\n",
    "    )\n",
    "\n",
    "    df_filtered.loc[mask_missing, 'AVG_QUALITY_BY_LG'] = df_filtered.loc[mask_missing, 'TRANSLATOR'].map(overall_avg)\n",
    "\n",
    "    # Para los traductores que no existen en data_df → asignar calidad por defecto (ej. 5)\n",
    "    df_filtered['AVG_QUALITY_BY_LG'] = df_filtered['AVG_QUALITY_BY_LG'].fillna(5)\n",
    "        \n",
    "    return df_filtered\n",
    "\n",
    "\n",
    "def compute_quality_by_task_type(data_df, df_filtered, task_type):\n",
    "    \"\"\"\n",
    "    Computes the average quality for each translator for a given task type.\n",
    "    If the translator has no experience with that task, falls back to:\n",
    "      - their overall average quality (with a penalty), or\n",
    "      - a 5 if no task has been done.\n",
    "    \n",
    "    Args:\n",
    "        df_filtered (pd.DataFrame): DataFrame with filtered translators.\n",
    "        task_type (str): The specific task type to evaluate.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: df_filtered with 'AVG_QUALITY_BY_TASK' and 'QUALITY_SOURCE_TASK'.\n",
    "    \"\"\"\n",
    "    if df_filtered.empty:\n",
    "        print(f\"Warning: No translators found in the filtered dataframe.\")\n",
    "        return df_filtered\n",
    "    \n",
    "    translators = df_filtered['TRANSLATOR'].unique()\n",
    "\n",
    "    # 1. Compute average quality for given task type\n",
    "    mask_task = (\n",
    "        (data_df['TASK_TYPE'] == task_type) &\n",
    "        (data_df['TRANSLATOR'].isin(translators))\n",
    "    )\n",
    "\n",
    "    avg_by_task = (\n",
    "        data_df[mask_task]\n",
    "        .groupby('TRANSLATOR')['QUALITY_EVALUATION']\n",
    "        .mean()\n",
    "        .round(2)\n",
    "    )\n",
    "\n",
    "    df_filtered['AVG_QUALITY_BY_TASK'] = df_filtered['TRANSLATOR'].map(avg_by_task)\n",
    "\n",
    "    # 2. Fallback to penalized overall average\n",
    "    mask_missing = df_filtered['AVG_QUALITY_BY_TASK'].isna()\n",
    "\n",
    "    overall_avg = (\n",
    "        data_df[data_df['TRANSLATOR'].isin(translators)]\n",
    "        .groupby('TRANSLATOR')['QUALITY_EVALUATION']\n",
    "        .mean()\n",
    "        .round(2)\n",
    "        .apply(lambda x: x - 1 if pd.notnull(x) else None)  # configurable penalization, for flexibility\n",
    "        #For a data-driven approach, use can standard deviation or percentile-based penalization to adapt to the distribution of quality scores\n",
    "    )\n",
    "\n",
    "    df_filtered.loc[mask_missing, 'AVG_QUALITY_BY_TASK'] = df_filtered.loc[mask_missing, 'TRANSLATOR'].map(overall_avg)\n",
    "\n",
    "    df_filtered['AVG_QUALITY_BY_TASK'] = df_filtered['AVG_QUALITY_BY_TASK'].fillna(5)\n",
    "\n",
    "    return df_filtered\n",
    "\n",
    "\n",
    "def compute_experience(df_filtered, task_type, source_lang, target_lang, industry, subindustry):\n",
    "    \"\"\"\n",
    "    Computes a soft experience score for each translator based on how many\n",
    "    dimensions match (task_type, language pair, industry, subindustry).\n",
    "\n",
    "    Args:\n",
    "        df_filtered (pd.DataFrame): Filtered translators' dataframe.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: With added column 'EXPERIENCE_SCORE'.\n",
    "    \"\"\"\n",
    "    TASK_TYPE_BONUS = {\n",
    "        'LanguageLead': 0.5,\n",
    "        'ProofReading': 0.5,\n",
    "        'Spotcheck': 0.5\n",
    "    }\n",
    "\n",
    "    translators = df_filtered['TRANSLATOR'].unique()\n",
    "\n",
    "    df = data_df[data_df['TRANSLATOR'].isin(translators)].copy()\n",
    "\n",
    "    # Base score: match on source, target, task_type\n",
    "    df['score'] = 0\n",
    "    df['score'] += (df['SOURCE_LANG'] == source_lang).astype(int)\n",
    "    df['score'] += (df['TARGET_LANG'] == target_lang).astype(int)\n",
    "    df['score'] += (df['TASK_TYPE'] == task_type).astype(int)\n",
    "\n",
    "    # Only add 1 point if industry or subindustry match\n",
    "    industry_match = (df['MANUFACTURER_INDUSTRY'] == industry)\n",
    "    subindustry_match = (df['MANUFACTURER_SUBINDUSTRY'] == subindustry)\n",
    "    df['score'] += ((industry_match | subindustry_match)).astype(int)\n",
    "\n",
    "    # Advanced task bonus\n",
    "    bonus_df = df[df['TASK_TYPE'].isin(TASK_TYPE_BONUS)].copy()\n",
    "    bonus_df['bonus'] = bonus_df['TASK_TYPE'].map(TASK_TYPE_BONUS)\n",
    "    bonus_scores = bonus_df.groupby('TRANSLATOR')['bonus'].sum()\n",
    "\n",
    "    # Base score\n",
    "    base_scores = df.groupby('TRANSLATOR')['score'].sum()\n",
    "\n",
    "    # Total experience = base + bonus\n",
    "    total_score = base_scores.add(bonus_scores, fill_value=0)\n",
    "\n",
    "    df_filtered['EXPERIENCE_SCORE'] = df_filtered['TRANSLATOR'].map(total_score).fillna(0)\n",
    "\n",
    "    # Normalize between 0 and 10\n",
    "    min_score = df_filtered['EXPERIENCE_SCORE'].min()\n",
    "    max_score = df_filtered['EXPERIENCE_SCORE'].max()\n",
    "\n",
    "    if max_score > min_score:\n",
    "        df_filtered['EXPERIENCE_SCORE'] = (\n",
    "            (df_filtered['EXPERIENCE_SCORE'] - min_score) / (max_score - min_score)\n",
    "        ) * 10\n",
    "    else:\n",
    "        df_filtered['EXPERIENCE_SCORE'] = 0\n",
    "\n",
    "    df_filtered['EXPERIENCE_SCORE'] = df_filtered['EXPERIENCE_SCORE'].round(2)\n",
    "\n",
    "    # Detect translators not present in data_df (no prior tasks)\n",
    "    missing_translators_mask = ~df_filtered['TRANSLATOR'].isin(data_df['TRANSLATOR'])\n",
    "    missing_translators = df_filtered.loc[missing_translators_mask, 'TRANSLATOR']\n",
    "\n",
    "    if not missing_translators.empty:\n",
    "        print(\"Translators with no experience data:\")\n",
    "        print(missing_translators.tolist())\n",
    "\n",
    "    # Compute average from those with scores\n",
    "    avg_experience = df_filtered.loc[~missing_translators_mask, 'EXPERIENCE_SCORE'].mean()\n",
    "    print(f\"Assigning average experience score of {round(avg_experience, 2)} to missing translators.\")\n",
    "\n",
    "    # Assign average to missing\n",
    "    df_filtered.loc[missing_translators_mask, 'EXPERIENCE_SCORE'] = avg_experience\n",
    "\n",
    "    return df_filtered\n",
    "\n",
    "\n",
    "def compute_experience_for_client(df_filtered, client):\n",
    "    \"\"\"\n",
    "    Computes an experience score for each translator based on a specific client\n",
    "\n",
    "    Args:\n",
    "        df_filtered (pd.DataFrame): Filtered translators' dataframe.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: With added column 'EXPERIENCE_CLIENT'.\n",
    "    \"\"\"\n",
    "    translators = df_filtered['TRANSLATOR'].unique()\n",
    "    df = data_df[data_df['TRANSLATOR'].isin(translators)].copy()\n",
    "\n",
    "\n",
    "    df['score'] = 0\n",
    "    df['score'] += (df['MANUFACTURER'] == client).astype(int)\n",
    "\n",
    "    # Total experience score = sum of weights per translator\n",
    "    experience_scores = df.groupby('TRANSLATOR')['score'].sum()\n",
    "\n",
    "    # Add to filtered dataframe\n",
    "    df_filtered['EXPERIENCE_CLIENT'] = df_filtered['TRANSLATOR'].map(experience_scores).fillna(0).astype(int)\n",
    "\n",
    "    # Normalizar entre 0 y 10\n",
    "    min_score = df_filtered['EXPERIENCE_CLIENT'].min()\n",
    "    max_score = df_filtered['EXPERIENCE_CLIENT'].max()\n",
    "\n",
    "    if max_score > min_score:  # Evitar división por 0\n",
    "        df_filtered['EXPERIENCE_CLIENT'] = ((df_filtered['EXPERIENCE_CLIENT'] - min_score) / (max_score - min_score)) * 10\n",
    "    else:\n",
    "        df_filtered['EXPERIENCE_CLIENT'] = 0  # Si todos los scores son iguales\n",
    "\n",
    "    df_filtered['EXPERIENCE_CLIENT'] = df_filtered['EXPERIENCE_CLIENT'].round(2)\n",
    "\n",
    "    return df_filtered\n",
    "\n",
    "\n",
    "def available_translators(task, translators_attributes_df, schedules_df, TRANSLATORS_UNAVAILABLE):\n",
    "    \"\"\"\n",
    "    Checks if translators are available for the task based on their weekly working schedule.\n",
    "    This, for now just takes into account the day of the week and the start time of the task. \n",
    "    TAKE INTO ACCOUNT: This can have problems if the translator is at the end of their weekly shedule, it also doesnt take into account multitasking.\n",
    "    \n",
    "    Args:\n",
    "        task (Task object): The task for which we want to check availability.\n",
    "        translators_attributes_df (pd.DataFrame): DataFrame containing the translators' attributes.\n",
    "        schedules_df (pd.DataFrame): DataFrame containing the weekly schedules of translators.\n",
    "        TRANSLATORS_UNAVAILABLE (list): List of translators who are unavailable.\n",
    "        \n",
    "    Returns:\n",
    "        df_filtered (pd.DataFrame): Filtered DataFrame containing translators who are available.\n",
    "    \"\"\"\n",
    "    ##TODO:check multitasking\n",
    "    # 1. Remove explicitly unavailable translators\n",
    "    df_filtered = translators_attributes_df[~translators_attributes_df['TRANSLATOR'].isin(TRANSLATORS_UNAVAILABLE)].copy()\n",
    "\n",
    "    # 2. Extract day of week and time from task\n",
    "    task_day = task.ASSIGNED.strftime('%a').upper()  #day of the week  e.g., 'MON', 'TUE'\n",
    "\n",
    "    # 3. Merge schedule info\n",
    "    df_filtered = df_filtered.merge(schedules_df, left_on='TRANSLATOR', right_on='NAME').drop(columns=['NAME'])\n",
    "\n",
    "    def is_available(row):\n",
    "        # 3.1 Check if works that day\n",
    "        if row[task_day] != 1:\n",
    "            return False\n",
    "\n",
    "        # 3.2 Parse working hours\n",
    "        task_start_time = timedelta(hours=task.ASSIGNED.hour, minutes=task.ASSIGNED.minute)\n",
    "        task_end_time = timedelta(hours=task.ASSIGNED.hour +1, minutes=task.ASSIGNED.minute) # Assuming 1 hour task duration \n",
    "        ## TODO change this to the estimated duration of the task based on std and mean of the task type\n",
    "\n",
    "        work_start_time = timedelta(hours=row['START'].hour, minutes=row['START'].minute)\n",
    "        work_end_time = timedelta(hours=row['END'].hour, minutes=row['END'].minute)\n",
    "        \n",
    "        return (task_start_time >= work_start_time) and (task_end_time < work_end_time) \n",
    "\n",
    "    # 4. Apply availability logic\n",
    "    df_filtered['IS_AVAILABLE'] = df_filtered.apply(is_available, axis=1)\n",
    "    df_filtered = df_filtered[df_filtered['IS_AVAILABLE'] == True].drop(columns=['IS_AVAILABLE'])\n",
    "\n",
    "    return df_filtered\n",
    "\n",
    "\n",
    "# ----- PRINCIPAL FUNCTION TO FILTER THE TRANSLATORS' ATTRIBUTES -----\n",
    "def filter_language_price_quality_availability(data_df, schedules_df, translators_attributes_df, task = Task, need_wildcard = False):\n",
    "    \"\"\"\n",
    "    Filters the translators' attributes by languages, price, quality and availability.\n",
    "    If need_wildcard is True, it will skip the filter corresponding to the wildcard.\n",
    "\n",
    "    Structured fallback:\n",
    "        Tries a strict filter.\n",
    "        If that fails, it retries with a wildcard (skipping one constraint).\n",
    "        If that also fails, it relaxes all filters except language pair. That's reasonable.\n",
    "    \n",
    "    Args:\n",
    "        translators_attributes_df (pd.DataFrame): \n",
    "            DataFrame containing the translators' attributes (name, languages, price, speed).\n",
    "        task (Task object): \n",
    "            The task for which we want to filter the translators.\n",
    "        need_wildcard (bool): \n",
    "            If True, skip the filter corresponding to the wildcard.\n",
    "            \n",
    "    Returns:\n",
    "        pd.DataFrame: \n",
    "            Filtered DataFrame containing translators who meet the criteria.\n",
    "    \"\"\"\n",
    "    \n",
    "    if not need_wildcard:\n",
    "        # Filter by language, price HARD FILTER\n",
    "        df_filtered = translators_attributes_df[\n",
    "            (translators_attributes_df['SOURCE_LANG'] == task.SOURCE_LANG) & \n",
    "            (translators_attributes_df['TARGET_LANG'] == task.TARGET_LANG) &\n",
    "            (translators_attributes_df['HOURLY_RATE'] <= task.SELLING_HOURLY_PRICE) \n",
    "        ].copy()\n",
    "\n",
    "        if df_filtered.empty:\n",
    "            print(f\"Warning: No translators found. Trying with wildcard...\")\n",
    "            return filter_language_price_quality_availability(data_df, schedules_df, translators_attributes_df, task = task, need_wildcard = True)\n",
    "\n",
    "        # add the average quality column\n",
    "        df_filtered = compute_quality_by_task_type(data_df, df_filtered, task_type=task.TASK_TYPE)\n",
    "        df_filtered = compute_quality_by_languages(data_df, df_filtered, source_lang=task.SOURCE_LANG, target_lang=task.TARGET_LANG)\n",
    "\n",
    "        df_filtered = df_filtered[df_filtered['AVG_QUALITY_BY_LG'] >= task.MIN_QUALITY]\n",
    "        df_filtered = df_filtered[df_filtered['AVG_QUALITY_BY_TASK'] >= task.MIN_QUALITY]\n",
    "\n",
    "        # Filter by availability\n",
    "        df_filtered = available_translators(task, df_filtered, schedules_df, TRANSLATORS_UNAVAILABLE)\n",
    "        \n",
    "        return df_filtered\n",
    "    \n",
    "    # same code as above but with the wildcard, it will skip the filter corresponding to the wildcard\n",
    "    else:\n",
    "        # if the wildcard is \"Price\", we don't filter by price\n",
    "        price_condition = (translators_attributes_df['HOURLY_RATE'] <= task.SELLING_HOURLY_PRICE) if task.WILDCARD != \"Price\" else True\n",
    "        # Filter by language, price \n",
    "        df_filtered = translators_attributes_df[\n",
    "            (translators_attributes_df['SOURCE_LANG'] == task.SOURCE_LANG) & \n",
    "            (translators_attributes_df['TARGET_LANG'] == task.TARGET_LANG) &\n",
    "            price_condition \n",
    "        ].copy()\n",
    "\n",
    "        # add the average quality column\n",
    "        df_filtered = compute_quality_by_languages(data_df, df_filtered, source_lang=task.SOURCE_LANG, target_lang=task.TARGET_LANG)\n",
    "        df_filtered = compute_quality_by_task_type(data_df, df_filtered, task_type=task.TASK_TYPE)\n",
    "\n",
    "        if task.WILDCARD != \"Quality\":\n",
    "            df_filtered = df_filtered[df_filtered['AVG_QUALITY_BY_LG'] >= task.MIN_QUALITY]\n",
    "            df_filtered = df_filtered[df_filtered['AVG_QUALITY_BY_TASK'] >= task.MIN_QUALITY]\n",
    "\n",
    "\n",
    "        if task.WILDCARD != \"Deadline\":\n",
    "            # Filter by availability\n",
    "            df_filtered = available_translators(task, df_filtered, schedules_df, TRANSLATORS_UNAVAILABLE)\n",
    "        \n",
    "        if df_filtered.empty:\n",
    "            print(\"No translators found even with wildcard. Relaxing all filters...\")\n",
    "            df_filtered = translators_attributes_df[\n",
    "                (translators_attributes_df['SOURCE_LANG'] == task.SOURCE_LANG) &\n",
    "                (translators_attributes_df['TARGET_LANG'] == task.TARGET_LANG)\n",
    "            ].copy()\n",
    "            # add the average quality column\n",
    "            df_filtered = compute_quality_by_languages(data_df, df_filtered, source_lang=task.SOURCE_LANG, target_lang=task.TARGET_LANG)\n",
    "            df_filtered = compute_quality_by_task_type(data_df, df_filtered, task_type=task.TASK_TYPE)\n",
    "            return df_filtered\n",
    "        \n",
    "        return df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nAt this stage, we have a filtered dataframe with the translators that are available for the task,\\nit contains the following columns:\\n    - TRANSLATOR: Name of the translator\\n    - SOURCE_LANG: Source language of the translator\\n    - TARGET_LANG: Target language of the translator\\n    - HOURLY_RATE: Hourly rate of the translator\\n    - Filtered by the availability\\n\\n    Things to take into account for the calculation of the scores:\\n    - AVG_QUALITY_BY_LNG: Average quality by language pair (if applicable)\\n    - AVG_QUALITY_BY_TASK: Average quality by task type (if applicable)\\n    - QUALITY_SOURCE_TASK: Source of the quality by task score (original, overall_penalized, global_penalized)\\n    - NUM_TASKS: Number of tasks performed by the translator (to take into account the reliability of the translator's quality and delay percentage, not experience because it is calculated based on specific tasks)\\n\\n    - EXPERIENCE_SCORE: Experience score based on task type, language pair, industry, and subindustry\\n    - EXPERIENCE_CLIENT: Experience score based on the specific client (if applicable)\\n    - AVG_DELAY_PERCENTAGE: Average delay percentage of the translator (if applicable)\\n\\n    \\nThere are some key considerations regarding the experience and quality weights:\\n    - Proofreading and Spotcheck need more expereinced translators.\\n    - LanguageLead is a more advanced task, so it needs more experience and quality.\\n    - Test should be assigned to the most experienced and high-quality TRANSLATOR for the client or topic, regardless of price.\\n    - For training we dont need to take into account the experience nor the quality\\n\\n\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "At this stage, we have a filtered dataframe with the translators that are available for the task,\n",
    "it contains the following columns:\n",
    "    - TRANSLATOR: Name of the translator\n",
    "    - SOURCE_LANG: Source language of the translator\n",
    "    - TARGET_LANG: Target language of the translator\n",
    "    - HOURLY_RATE: Hourly rate of the translator\n",
    "    - Filtered by the availability\n",
    "\n",
    "    Things to take into account for the calculation of the scores:\n",
    "    - AVG_QUALITY_BY_LNG: Average quality by language pair (if applicable)\n",
    "    - AVG_QUALITY_BY_TASK: Average quality by task type (if applicable)\n",
    "    - QUALITY_SOURCE_TASK: Source of the quality by task score (original, overall_penalized, global_penalized)\n",
    "    - NUM_TASKS: Number of tasks performed by the translator (to take into account the reliability of the translator's quality and delay percentage, not experience because it is calculated based on specific tasks)\n",
    "\n",
    "    - EXPERIENCE_SCORE: Experience score based on task type, language pair, industry, and subindustry\n",
    "    - EXPERIENCE_CLIENT: Experience score based on the specific client (if applicable)\n",
    "    - AVG_DELAY_PERCENTAGE: Average delay percentage of the translator (if applicable)\n",
    "\n",
    "    \n",
    "There are some key considerations regarding the experience and quality weights:\n",
    "    - Proofreading and Spotcheck need more expereinced translators.\n",
    "    - LanguageLead is a more advanced task, so it needs more experience and quality.\n",
    "    - Test should be assigned to the most experienced and high-quality TRANSLATOR for the client or topic, regardless of price.\n",
    "    - For training we dont need to take into account the experience nor the quality\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- KNN ----\n",
    "def knn(df_filtered, task, metric = 'euclidean', need_wildcard=False):\n",
    "    \"\"\"\n",
    "    Optimized KNN to find the best translators based on the task's requirements.\n",
    "    \n",
    "    Args:\n",
    "        df_filtered (pd.DataFrame): DataFrame containing the filtered translators' attributes.\n",
    "        task (Task object): The task for which we are finding suitable translators.\n",
    "        need_wildcard (bool): Whether to ignore the wildcard feature in KNN calculation.\n",
    "\n",
    "    Returns:\n",
    "        distances (np.ndarray): Distances of the nearest neighbors.\n",
    "        indexes (np.ndarray): Indices of the nearest neighbors in the original DataFrame.\n",
    "    \"\"\"\n",
    "    # Define features for the KNN\n",
    "    features = ['HOURLY_RATE', 'AVG_QUALITY_BY_LG', 'AVG_QUALITY_BY_TASK', 'AVG_DELAY_PERCENTAGE', 'EXPERIENCE_SCORE', 'EXPERIENCE_CLIENT']\n",
    "    ideal_values = [1, 10, 10, -100, 10, 10]  # Ideal values for the features (price, quality, speed, experience), this is like the ideal translator\n",
    "    \n",
    "    # Initialize weights (weights for each feature)\n",
    "    weights = np.array([1, 1.5, 1.5, 0.25, 1, 0.5])  # Default weights for the features\n",
    "\n",
    "    # Adjust weights based on task type\n",
    "    if task.TASK_TYPE == 'ProofReading' or task.TASK_TYPE == 'Spotcheck':\n",
    "        # Higher weight for experience and quality\n",
    "        weights[4] *= 2  # Experience score weight increased\n",
    "\n",
    "    elif task.TASK_TYPE == 'LanguageLead':\n",
    "        # LanguageLead requires more experience and quality\n",
    "        weights[4] *= 2\n",
    "        weights[1] *= 2\n",
    "        weights[2] *= 2\n",
    "\n",
    "    elif task.TASK_TYPE == 'Test':\n",
    "        # Test task needs the highest experience and quality, ignore price\n",
    "        weights[0] = 0  # Price doesn't matter for Test\n",
    "        weights[1] *= 2  # Quality by language pair weight increased\n",
    "        weights[2] *= 2  # Quality by task weight increased\n",
    "        weights[4] *= 2  # Experience score weight increased\n",
    "        weights[3] *= 3  # Experience score  for client weight increased\n",
    "\n",
    "    elif task.TASK_TYPE == 'Training':\n",
    "        # Training task doesn't consider experience or quality\n",
    "        weights[1] = 0  # Set quality to 0\n",
    "        weights[2] = 0  # Set quality to 0\n",
    "        weights[3] = 0  # Set experience to 0\n",
    "        weights[4] = 0  # Set experience to 0\n",
    "\n",
    "    # If wildcard is required, adjust the weights based on the wildcard preference\n",
    "    if not need_wildcard:\n",
    "        wildcard_vector = np.ones_like(weights)\n",
    "\n",
    "        if task.WILDCARD == 'Price':\n",
    "            wildcard_vector[0] = 0.25  # Price becomes less important\n",
    "        elif task.WILDCARD == 'Quality':\n",
    "            wildcard_vector[1] = 0.25  # Quality becomes less important\n",
    "        elif task.WILDCARD == 'Deadline':\n",
    "            wildcard_vector[3] = 0.25  # Deadline (Avg. Delay) becomes less important\n",
    "\n",
    "        weights = weights * wildcard_vector  # Apply adjusted weights based on wildcard\n",
    "\n",
    "    # Select features for the KNN calculation\n",
    "    X = df_filtered[features]\n",
    "\n",
    "    # Standardize the features (scaling)\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    X_weighted = X_scaled * weights  # Apply the weights to scaled features\n",
    "\n",
    "    # Initialize and train the KNN model\n",
    "    knn = NearestNeighbors(metric=metric)\n",
    "    knn.fit(X_weighted)\n",
    "\n",
    "    # Define ideal task values as a DataFrame\n",
    "    task_df = pd.DataFrame([ideal_values], columns=features)\n",
    "    task_scaled = scaler.transform(task_df)\n",
    "    task_weighted = task_scaled * weights  # Weight the task ideal values too\n",
    "\n",
    "    # Find the nearest neighbors based on the task's ideal values\n",
    "    distances, indexes = knn.kneighbors(task_weighted, n_neighbors=len(df_filtered))  # Adjust the number of neighbors as needed\n",
    "\n",
    "    return distances, indexes\n",
    "\n",
    "\n",
    "def get_best_translators(df_filtered, indexes, distances):\n",
    "    \"\"\"\n",
    "    Get the best translators based on the KNN results.\n",
    "    \n",
    "    Args:\n",
    "        df_filtered (pd.DataFrame): \n",
    "            Contains the filtered translators' attributes (name, language, price, quality, speed).\n",
    "        indexes (np.ndarray): \n",
    "            Indices of the nearest neighbors in the df_filtered.\n",
    "        distances (np.ndarray): \n",
    "            Distances of the nearest neighbors.\n",
    "            \n",
    "    Returns:\n",
    "        df_filtered (pd.DataFrame): \n",
    "            Contains the filtered translators' attributes (name, language, price, quality, speed AND similarity_score).\n",
    "    \"\"\"\n",
    "    \n",
    "    selected_translators = df_filtered.iloc[indexes[0]].copy()\n",
    "    \n",
    "    # Add the similarity score\n",
    "    selected_translators['Similarity Score'] = distances[0].round(2)  # Round to 2 decimal places\n",
    "\n",
    "    # Sort by similarity score (ascending: closest match first)\n",
    "    selected_translators = selected_translators.sort_values(by='Similarity Score', ascending=False) \n",
    "\n",
    "    return selected_translators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_metrics(df_filtered, task):\n",
    "    metrics = ['euclidean', 'manhattan']\n",
    "    results = {}\n",
    "\n",
    "    for metric in metrics:\n",
    "        distances, indexes = knn(df_filtered, task, metric=metric, need_wildcard=False)\n",
    "        selected_translators = get_best_translators(df_filtered, indexes, distances)\n",
    "        \n",
    "        # Guarda los resultados para cada métrica\n",
    "        results[metric] = selected_translators\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def plot_similarity(results):\n",
    "    \"\"\"\n",
    "    Plot the similarity scores for translators selected using different distance metrics.\n",
    "    \n",
    "    Args:\n",
    "        results (dict): A dictionary where keys are metric names and values are DataFrames \n",
    "                        with the selected translators' similarity scores.\n",
    "    \"\"\"\n",
    "    # Create a DataFrame to hold all similarity scores\n",
    "    similarity_data = []\n",
    "    for metric, translators in results.items():\n",
    "        # Add a new column with the metric name\n",
    "        translators['Metric'] = metric\n",
    "        similarity_data.append(translators)\n",
    "\n",
    "    # Concatenate all DataFrames\n",
    "    all_similarity_scores = pd.concat(similarity_data)\n",
    "\n",
    "    # Plot using seaborn\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.boxplot(x='Metric', y='Similarity Score', data=all_similarity_scores)\n",
    "    plt.title('Comparison of Similarity Scores for Different Metrics')\n",
    "    plt.xlabel('Distance Metric')\n",
    "    plt.ylabel('Similarity Score')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cosine similaruty is not considered, as it measures how similarly two vectors point in the same direction, regardless of their magnitude, and is used when relative proportions matter more than absolute values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------ DEBUGGGING ------------\n",
    "# # Creates a dataframe with  the additional attributes\n",
    "# translators_attributes_df = compute_delay_percentage(train_df, transl_cost_pairs_df)\n",
    "# translators_attributes_df = compute_number_tasks(train_df, translators_attributes_df) \n",
    "\n",
    "# task_row = validation_df_clean.iloc[0]\n",
    "# #Convert the current row to a task\n",
    "# new_task = task_row.copy()\n",
    "# new_task = new_task.rename({'HOURLY_RATE': 'SELLING_HOURLY_PRICE', 'QUALITY_EVALUATION': 'MIN_QUALITY'})\n",
    "\n",
    "# match = clients_df[clients_df['CLIENT_NAME'].str.strip() == new_task['MANUFACTURER'].strip()]\n",
    "\n",
    "# if not match.empty:\n",
    "#     new_task['WILDCARD'] = match.iloc[0]['WILDCARD']\n",
    "#     new_task['HOURLY_RATE'] = match.iloc[0]['SELLING_HOURLY_PRICE']\n",
    "#     new_task['QUALITY_EVALUATION'] = match.iloc[0]['MIN_QUALITY']\n",
    "# else:\n",
    "#     print(\"WARNING: No match found in schedules_df for the given client. Setting default values.\")\n",
    "#     new_task['WILDCARD'] = 'Quality'\n",
    "#     new_task['HOURLY_RATE'] = new_task['SELLING_HOURLY_PRICE']\n",
    "#     new_task['QUALITY_EVALUATION'] = new_task['MIN_QUALITY']\n",
    "\n",
    "# new_task = Task(**new_task.to_dict())  # Convert the task to the Task object\n",
    "\n",
    "# df_filtered = translators_attributes_df[\n",
    "#             (translators_attributes_df['SOURCE_LANG'] == new_task.SOURCE_LANG) & \n",
    "#             (translators_attributes_df['TARGET_LANG'] == new_task.TARGET_LANG) &\n",
    "#             (translators_attributes_df['HOURLY_RATE'] <= new_task.SELLING_HOURLY_PRICE) \n",
    "#         ].copy()\n",
    "\n",
    "# df_filtered = compute_quality_by_languages(data_df, df_filtered, source_lang=new_task.SOURCE_LANG, target_lang=new_task.TARGET_LANG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assigning average experience score of 2.4 to missing translators.\n",
      "Warning: No translators found. Trying with wildcard...\n",
      "Warning: No translators found in the filtered dataframe.\n",
      "Warning: No translators found in the filtered dataframe.\n",
      "No translators found even with wildcard. Relaxing all filters...\n",
      "Assigning average experience score of 0.64 to missing translators.\n",
      "Warning: No translators found. Trying with wildcard...\n",
      "Assigning average experience score of 3.11 to missing translators.\n",
      "Assigning average experience score of 1.13 to missing translators.\n",
      "Assigning average experience score of 0.92 to missing translators.\n",
      "Assigning average experience score of 1.05 to missing translators.\n",
      "Assigning average experience score of 5.0 to missing translators.\n",
      "Assigning average experience score of 2.0 to missing translators.\n",
      "Assigning average experience score of 1.24 to missing translators.\n",
      "No translators found for task 10325876, English → Spanish (LA) . Skipping...\n",
      "Assigning average experience score of nan to missing translators.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0, 6)) while a minimum of 1 is required by StandardScaler.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 42\u001b[0m\n\u001b[0;32m     39\u001b[0m compute_experience_for_client(df_filtered, client\u001b[38;5;241m=\u001b[39mnew_task\u001b[38;5;241m.\u001b[39mMANUFACTURER)\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Get the distances and indexes from KNN\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m distances, indexes \u001b[38;5;241m=\u001b[39m knn(df_filtered, new_task)\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# Get the best translators\u001b[39;00m\n\u001b[0;32m     45\u001b[0m selected_translators \u001b[38;5;241m=\u001b[39m get_best_translators(df_filtered, indexes, distances)\n",
      "Cell \u001b[1;32mIn[31], line 66\u001b[0m, in \u001b[0;36mknn\u001b[1;34m(df_filtered, task, metric, need_wildcard)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m# Standardize the features (scaling)\u001b[39;00m\n\u001b[0;32m     65\u001b[0m scaler \u001b[38;5;241m=\u001b[39m StandardScaler()\n\u001b[1;32m---> 66\u001b[0m X_scaled \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mfit_transform(X)\n\u001b[0;32m     67\u001b[0m X_weighted \u001b[38;5;241m=\u001b[39m X_scaled \u001b[38;5;241m*\u001b[39m weights  \u001b[38;5;66;03m# Apply the weights to scaled features\u001b[39;00m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;66;03m# Initialize and train the KNN model\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\34642\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:295\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 295\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    296\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    297\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    298\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    299\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    300\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    301\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\34642\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1098\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m   1083\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1084\u001b[0m             (\n\u001b[0;32m   1085\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis object (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) has a `transform`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1093\u001b[0m             \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[0;32m   1094\u001b[0m         )\n\u001b[0;32m   1096\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1097\u001b[0m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[1;32m-> 1098\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1100\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[0;32m   1101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32mc:\\Users\\34642\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:876\u001b[0m, in \u001b[0;36mStandardScaler.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    874\u001b[0m \u001b[38;5;66;03m# Reset internal state before fitting\u001b[39;00m\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[1;32m--> 876\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpartial_fit(X, y, sample_weight)\n",
      "File \u001b[1;32mc:\\Users\\34642\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1472\u001b[0m     )\n\u001b[0;32m   1473\u001b[0m ):\n\u001b[1;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\34642\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:912\u001b[0m, in \u001b[0;36mStandardScaler.partial_fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    880\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Online computation of mean and std on X for later scaling.\u001b[39;00m\n\u001b[0;32m    881\u001b[0m \n\u001b[0;32m    882\u001b[0m \u001b[38;5;124;03mAll of X is processed as a single batch. This is intended for cases\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    909\u001b[0m \u001b[38;5;124;03m    Fitted scaler.\u001b[39;00m\n\u001b[0;32m    910\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    911\u001b[0m first_call \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_samples_seen_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 912\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[0;32m    913\u001b[0m     X,\n\u001b[0;32m    914\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsc\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    915\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mFLOAT_DTYPES,\n\u001b[0;32m    916\u001b[0m     force_all_finite\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    917\u001b[0m     reset\u001b[38;5;241m=\u001b[39mfirst_call,\n\u001b[0;32m    918\u001b[0m )\n\u001b[0;32m    919\u001b[0m n_features \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    921\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\34642\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:633\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    631\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 633\u001b[0m     out \u001b[38;5;241m=\u001b[39m check_array(X, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[0;32m    635\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[1;32mc:\\Users\\34642\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1072\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m   1070\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n\u001b[0;32m   1071\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_samples \u001b[38;5;241m<\u001b[39m ensure_min_samples:\n\u001b[1;32m-> 1072\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1073\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m sample(s) (shape=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m) while a\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1074\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m minimum of \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m is required\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1075\u001b[0m             \u001b[38;5;241m%\u001b[39m (n_samples, array\u001b[38;5;241m.\u001b[39mshape, ensure_min_samples, context)\n\u001b[0;32m   1076\u001b[0m         )\n\u001b[0;32m   1078\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_features \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m   1079\u001b[0m     n_features \u001b[38;5;241m=\u001b[39m array\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[1;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0, 6)) while a minimum of 1 is required by StandardScaler."
     ]
    }
   ],
   "source": [
    "# ----- MAIN CODE ----\n",
    "# Creates a dataframe with  the additional attributes\n",
    "translators_attributes_df = compute_delay_percentage(train_df, transl_cost_pairs_df)\n",
    "translators_attributes_df = compute_number_tasks(train_df, translators_attributes_df) \n",
    "\n",
    "# Define the top-k value for evaluation\n",
    "k = 10\n",
    "correct_predictions = 0\n",
    "total_predictions = 0\n",
    "\n",
    "# Iterate over each row in the validation set\n",
    "for idx, task_row in validation_df_clean.iterrows():\n",
    "    #Convert the current row to a task\n",
    "    new_task = task_row.copy()\n",
    "    new_task = new_task.rename({'HOURLY_RATE': 'SELLING_HOURLY_PRICE', 'QUALITY_EVALUATION': 'MIN_QUALITY'})\n",
    "    \n",
    "    match = clients_df[clients_df['CLIENT_NAME'].str.strip() == new_task['MANUFACTURER'].strip()]\n",
    "\n",
    "    if not match.empty:\n",
    "        new_task['WILDCARD'] = match.iloc[0]['WILDCARD']\n",
    "        new_task['HOURLY_RATE'] = match.iloc[0]['SELLING_HOURLY_PRICE']\n",
    "        new_task['QUALITY_EVALUATION'] = match.iloc[0]['MIN_QUALITY']\n",
    "    else:\n",
    "        print(\"WARNING: No match found in schedules_df for the given client. Setting default values.\")\n",
    "        new_task['WILDCARD'] = 'Quality'\n",
    "        new_task['HOURLY_RATE'] = new_task['SELLING_HOURLY_PRICE']\n",
    "        new_task['QUALITY_EVALUATION'] = new_task['MIN_QUALITY']\n",
    "    \n",
    "    new_task = Task(**new_task.to_dict())  # Convert the task to the Task object\n",
    "    \n",
    "    # Filter translators based on task attributes\n",
    "    df_filtered = filter_language_price_quality_availability(data_df, schedules_df, translators_attributes_df, new_task)\n",
    "\n",
    "    if df_filtered.empty:\n",
    "        print(f\"No translators found for task {new_task.TASK_ID}, {new_task.SOURCE_LANG} → {new_task.TARGET_LANG} . Skipping...\")\n",
    "\n",
    "    # Compute experience scores for the filtered translators\n",
    "    compute_experience(df_filtered, task_type=new_task.TASK_TYPE, source_lang=new_task.SOURCE_LANG, target_lang=new_task.TARGET_LANG, industry=new_task.MANUFACTURER_INDUSTRY, subindustry=new_task.MANUFACTURER_SUBINDUSTRY)\n",
    "    compute_experience_for_client(df_filtered, client=new_task.MANUFACTURER)\n",
    "\n",
    "    # Get the distances and indexes from KNN\n",
    "    distances, indexes = knn(df_filtered, new_task)\n",
    "    \n",
    "    # Get the best translators\n",
    "    selected_translators = get_best_translators(df_filtered, indexes, distances)\n",
    "    \n",
    "    # Retrieve the true translator label for the current task\n",
    "    true_translator = validation_translator_labels[idx]  # Assuming you have the true labels in the dictionary from earlier\n",
    "    \n",
    "    # Check if any of the top-k translators match the true translator\n",
    "    top_k_translators = selected_translators.iloc[:k]  # Top-k translators\n",
    "    if true_translator in top_k_translators['TRANSLATOR'].values:\n",
    "        correct_predictions += 1\n",
    "    \n",
    "    total_predictions += 1\n",
    "\n",
    "# Calculate the top-k accuracy\n",
    "top_k_accuracy = correct_predictions / total_predictions\n",
    "print(f\"Top-{k} Accuracy: {top_k_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ID                                       219619\n",
      "PM                                                  PMT\n",
      "TASK_ID                                        11055080\n",
      "START                               2021-09-23 11:50:00\n",
      "END                                 2021-09-23 17:00:00\n",
      "TASK_TYPE                                   Translation\n",
      "SOURCE_LANG                                     English\n",
      "TARGET_LANG                                      Basque\n",
      "ASSIGNED                            2021-09-23 11:52:12\n",
      "READY                               2021-09-23 11:52:19\n",
      "WORKING                             2021-09-23 14:10:52\n",
      "DELIVERED                           2021-09-23 16:58:51\n",
      "RECEIVED                            2021-09-23 16:59:37\n",
      "CLOSE                               2021-09-23 17:00:36\n",
      "FORECAST                                           0.18\n",
      "SELLING_HOURLY_PRICE                                 33\n",
      "COST                                               5.94\n",
      "MIN_QUALITY                                           8\n",
      "MANUFACTURER                                    NetWorx\n",
      "MANUFACTURER_SECTOR              Information Technology\n",
      "MANUFACTURER_INDUSTRY_GROUP    Communications Equipment\n",
      "MANUFACTURER_INDUSTRY          Communications Equipment\n",
      "MANUFACTURER_SUBINDUSTRY       Communications Equipment\n",
      "Name: 464732, dtype: object\n"
     ]
    }
   ],
   "source": [
    "new_task = validation_df_clean.loc[validation_df_clean['TASK_ID'] == 10189682].iloc[0]\n",
    "# Convert the current row to a task\n",
    "new_task = task_row.copy()\n",
    "new_task = new_task.rename({'HOURLY_RATE': 'SELLING_HOURLY_PRICE', 'QUALITY_EVALUATION': 'MIN_QUALITY'})\n",
    "\n",
    "print(new_task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of mistakes in the dataset they have been mostly added to the analysis to keep things clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "one of them is that some of the translators (13) finish after 0am "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hay algunos con fecha de START == END y si START > ASSIGNED (son errores porque no tiene sentido); pero si START <= ASSIGNED, \n",
    "# probablemente signifique que END (deadline) era urgente y no puso fecha límite, no son errores\n",
    "\n",
    "\n",
    "# provisionalmente cambiamos el START como ASSIGNED para estos casos\n",
    "# data_df.loc[data_df['START'] > data_df['ASSIGNED'], 'START'] = data_df['ASSIGNED'] \n",
    "\n",
    "## TODO we have to discuss this further, it have been added to data analysis but not shure how to address it "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next steps\n",
    "actualize the unavailable translators:\n",
    "- add it to the list when one is selected by the client in the forntend\n",
    "- remove it from the list when the task has been finished "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
