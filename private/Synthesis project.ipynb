{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'cleaning_utils' from 'c:\\\\Users\\\\34642\\\\OneDrive\\\\Escritorio\\\\Synthesis Project I\\\\Notebooks\\\\../utils\\\\cleaning_utils.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "from datetime import timedelta\n",
    "\n",
    "from matplotlib import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import importlib\n",
    "\n",
    "sys.path.append('../utils') \n",
    "import cleaning_utils as cu\n",
    "importlib.reload(cu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environmental Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "vP_0tHxQo39T"
   },
   "outputs": [],
   "source": [
    "data_path = os.path.join(\"..\", \"Data\")\n",
    "schedules_df = pd.read_excel(os.path.join(data_path, \"Schedules.xlsx\"))\n",
    "data_df = pd.read_excel(os.path.join(data_path, \"Data.xlsx\"))\n",
    "clients_df = pd.read_excel(os.path.join(data_path, \"Clients.xlsx\"))\n",
    "transl_cost_pairs_df = pd.read_excel(os.path.join(data_path, \"TranslatorsCost+Pairs.xlsx\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df['START'] = pd.to_datetime(data_df['START'], errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Envirinmental Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "\n",
    "TRANSLATORS_UNAVAILABLE = []\n",
    "\n",
    "wildcards = [None, \"Quality\", \"Time\", \"Cost\"]\n",
    "task_types = data_df[\"TASK_TYPE\"].unique()\n",
    "unique_language_pairs = data_df[[\"SOURCE_LANG\", \"TARGET_LANG\"]].drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note:** `TRANSLATORS_UNAVAILABLE` is a list for keeping track of translators alredy assigned or performing a task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping invalid rows in column 'START'...\n",
      "No invalid dates found in column 'END'.\n",
      "No invalid dates found in column 'DELIVERED'.\n",
      "No invalid dates found in column 'ASSIGNED'.\n",
      " \n",
      "Invalid START dates:\n",
      " 91127   NaT\n",
      "Name: START, dtype: datetime64[ns] \n",
      "\n",
      "Invalid END dates:\n",
      " Series([], Name: END, dtype: datetime64[ns]) \n",
      "\n",
      "Invalid DELIVERED dates:\n",
      " Series([], Name: ASSIGNED, dtype: datetime64[ns]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Function to detect invalid rows\n",
    "data_df, start_invalid_dates = cu.drop_invalid_dates(data_df, 'START')\n",
    "data_df, end_invalid_dates = cu.drop_invalid_dates(data_df, 'END')\n",
    "data_df, delivered_invalid_dates = cu.drop_invalid_dates(data_df, 'DELIVERED')\n",
    "data_df, delivered_invalid_dates = cu.drop_invalid_dates(data_df, 'ASSIGNED')\n",
    "\n",
    "print(\" \")\n",
    "\n",
    "# Show the invalid dates in each column\n",
    "print(\"Invalid START dates:\\n\", start_invalid_dates, \"\\n\")\n",
    "print(\"Invalid END dates:\\n\", end_invalid_dates, \"\\n\")\n",
    "print(\"Invalid DELIVERED dates:\\n\", delivered_invalid_dates, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 25764 rows with missing values or START >= END.\n"
     ]
    }
   ],
   "source": [
    "data_df = cu.drop_invalid_rows(data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task Information\n",
    "\n",
    "- **PROJECT_ID**: Project code (additional info, likely not necessary).\n",
    "- **PM**: Responsible management team.\n",
    "- **TASK_ID**: Task code.\n",
    "- **TRANSLATOR**: Translator responsible for the task.\n",
    "-----\n",
    "- **START**: Task start date.\n",
    "- **END**: Theoretical task delivery date (can be compared with `DELIVERED` to check for delays).\n",
    "-----\n",
    "Some considerations must be taken into account:\n",
    "- **DTP**: Desktop-Publishing tasks.\n",
    "- **Engineering**: Engineering tasks such as file conversions, coding, etc.\n",
    "- **LanguageLead**: Linguistic management tasks. Assigned to highly experienced and quality-oriented individuals who regularly work on the project.\n",
    "- **Management**: General management tasks.\n",
    "- **Miscellaneous**: Various linguistic tasks.\n",
    "- **PostEditing**: Post-editing tasks. Similar to Translation tasks but with slightly different skills required for the TRANSLATOR.\n",
    "- **ProofReading**: Full review of a Translation or PostEditing. Always follows a Translation or PostEditing. The TRANSLATOR assigned must have more \n",
    "experience than the person who performed the initial step.\n",
    "- **Spotcheck**: Partial review of a Translation or PostEditing. Similar conditions as ProofReading.\n",
    "- **TEST**: Test required to qualify for working with a client. Should be assigned to the most experienced and high-quality TRANSLATOR \n",
    "for the client or topic, regardless of price but considering the deadline.\n",
    "- **Training**: Translator experience and quality are not considered.\n",
    "- **Translation**: Translation task. The translator’s quality can be slightly lower if the ProofReading (not Spotcheck) is done by a superior. If \n",
    "Spotcheck is done, the required quality must be met.\n",
    "-----\n",
    "- **SOURCE_LANG**: Source language.\n",
    "- **TARGET_LANG**: Target language.\n",
    "-----\n",
    "- **ASSIGNED**: Time when the task is assigned (pre-notice) to the TRANSLATOR.\n",
    "- **READY**: Time when the TRANSLATOR is notified they can start.\n",
    "- **WORKING**: Time when the TRANSLATOR starts the task.\n",
    "- **DELIVERED**: Time when the TRANSLATOR delivers the task.\n",
    "- **RECEIVED**: Time when the PM receives the task.\n",
    "- **CLOSE**: Time when the PM marks the task as completed.\n",
    "-----\n",
    "- **FORECAST**: Estimated hours for completion.\n",
    "- **HOURLY_RATE**: Task hourly rate.\n",
    "- **COST**: Total task cost.\n",
    "- **QUALITY_EVALUATION**: Quality control evaluation.\n",
    "-----\n",
    "- **MANUFACTURER**: Client.\n",
    "- **MANUFACTURER_SECTOR**: Level 1 client categorization.\n",
    "- **MANUFACTURER_INDUSTRY_GROUP**: Level 2 client categorization.\n",
    "- **MANUFACTURER_INDUSTRY**: Level 3 client categorization.\n",
    "- **MANUFACTURER_SUBINDUSTRY**: Level 4 client categorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Task:\n",
    "    def __init__(self, **kwargs):\n",
    "        \"\"\"\n",
    "        A class used to represent a Task. It initializes the attributes dynamically \n",
    "        using the keyword arguments passed. Default values are provided for certain fields.\n",
    "        \"\"\"\n",
    "        self.PROJECT_ID = kwargs.get('PROJECT_ID', None)\n",
    "        self.TASK_ID = kwargs.get('TASK_ID', None)\n",
    "        self.ASSIGNED = kwargs.get('ASSIGNED', None)\n",
    "        self.END = kwargs.get('END', None)\n",
    "        self.SELLING_HOURLY_PRICE = kwargs.get('SELLING_HOURLY_PRICE', None)\n",
    "        self.MIN_QUALITY = kwargs.get('MIN_QUALITY', None)\n",
    "        self.WILDCARD = kwargs.get('WILDCARD', None) \n",
    "        self.TASK_TYPE = kwargs.get('TASK_TYPE', None)\n",
    "        self.SOURCE_LANG = kwargs.get('SOURCE_LANG', None)\n",
    "        self.TARGET_LANG = kwargs.get('TARGET_LANG', None)\n",
    "        self.MANUFACTURER = kwargs.get('MANUFACTURER', None)\n",
    "        self.MANUFACTURER_SECTOR = kwargs.get('MANUFACTURER_SECTOR', None)\n",
    "        self.MANUFACTURER_INDUSTRY_GROUP = kwargs.get('MANUFACTURER_INDUSTRY_GROUP', None)\n",
    "        self.MANUFACTURER_INDUSTRY = kwargs.get('MANUFACTURER_INDUSTRY', None)\n",
    "        self.MANUFACTURER_SUBINDUSTRY = kwargs.get('MANUFACTURER_SUBINDUSTRY', None)\n",
    "        \n",
    "        # Optional attributes with None default value\n",
    "        self.START = kwargs.get('START', None)\n",
    "        self.PM = kwargs.get('PM', None)\n",
    "        self.TRANSLATOR = kwargs.get('TRANSLATOR', None)\n",
    "        self.READY = kwargs.get('READY', None)\n",
    "        self.WORKING = kwargs.get('WORKING', None)\n",
    "        self.DELIVERED = kwargs.get('DELIVERED', None)\n",
    "        self.RECEIVED = kwargs.get('RECEIVED', None)\n",
    "        self.CLOSE = kwargs.get('CLOSE', None)\n",
    "        self.FORECAST = kwargs.get('FORECAST', None)\n",
    "        self.HOURLY_RATE = kwargs.get('HOURLY_RATE', None)\n",
    "        self.COST = kwargs.get('COST', None)\n",
    "        self.QUALITY_EVALUATION = kwargs.get('QUALITY_EVALUATION', None)\n",
    "        \n",
    "    \n",
    "    def __str__(self):\n",
    "        return (\n",
    "            f\"Task Details:\\n\"\n",
    "            f\"  - Task ID: {self.TASK_ID}\\n\"\n",
    "            f\"  - Type: {self.TASK_TYPE}\\n\"\n",
    "            f\"  - Client: {self.MANUFACTURER}\\n\"\n",
    "            f\"  - Sector: {self.MANUFACTURER_SECTOR}\\n\"\n",
    "            f\"  - Industry (Subsector): {self.MANUFACTURER_INDUSTRY}\\n\"\n",
    "            f\"  - Start: {self.START}\\n\"\n",
    "            f\"  - Budget: {self.SELLING_HOURLY_PRICE}\\n\"\n",
    "            f\"  - Quality: {self.MIN_QUALITY}\\n\"\n",
    "            f\"  - Wildcard: {self.WILDCARD}\\n\"\n",
    "            f\"  - Source Language: {self.SOURCE_LANG}\\n\"\n",
    "            f\"  - Target Language: {self.TARGET_LANG}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and validation (e.g., 80% train, 20% validation)\n",
    "train_df, validation_df = train_test_split(data_df, test_size=0.2, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_and_save_translator_labels(df, translator_column=\"TRANSLATOR\"):\n",
    "    \"\"\"\n",
    "    Extracts and removes translators from test_df, then saves them as a label dict.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The dataframe containing translator data.\n",
    "        translator_column (str): The column name that holds the translator labels.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: The test_df without the translators column.\n",
    "        dict: Dictionary of translator labels {index: translators}\n",
    "    \"\"\"\n",
    "    if translator_column not in df.columns:\n",
    "        raise ValueError(f\"Column '{translator_column}' not found in test_df.\")\n",
    "    \n",
    "    # Extract labels\n",
    "    translator_labels = df[translator_column].to_dict()\n",
    "    \n",
    "    # Drop the column from the DataFrame\n",
    "    df = df.drop(columns=[translator_column])\n",
    "    \n",
    "    return df, translator_labels\n",
    "\n",
    "train_df_clean, train_translator_labels = drop_and_save_translator_labels(train_df)\n",
    "validation_df_clean, validation_translator_labels = drop_and_save_translator_labels(validation_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PROJECT_ID</th>\n",
       "      <th>PM</th>\n",
       "      <th>TASK_ID</th>\n",
       "      <th>START</th>\n",
       "      <th>END</th>\n",
       "      <th>TASK_TYPE</th>\n",
       "      <th>SOURCE_LANG</th>\n",
       "      <th>TARGET_LANG</th>\n",
       "      <th>ASSIGNED</th>\n",
       "      <th>READY</th>\n",
       "      <th>WORKING</th>\n",
       "      <th>DELIVERED</th>\n",
       "      <th>RECEIVED</th>\n",
       "      <th>CLOSE</th>\n",
       "      <th>FORECAST</th>\n",
       "      <th>HOURLY_RATE</th>\n",
       "      <th>COST</th>\n",
       "      <th>QUALITY_EVALUATION</th>\n",
       "      <th>MANUFACTURER</th>\n",
       "      <th>MANUFACTURER_SECTOR</th>\n",
       "      <th>MANUFACTURER_INDUSTRY_GROUP</th>\n",
       "      <th>MANUFACTURER_INDUSTRY</th>\n",
       "      <th>MANUFACTURER_SUBINDUSTRY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>106193</th>\n",
       "      <td>212576</td>\n",
       "      <td>PMT</td>\n",
       "      <td>10313113</td>\n",
       "      <td>2014-11-06 15:29:00</td>\n",
       "      <td>2014-11-10 09:00:00</td>\n",
       "      <td>Translation</td>\n",
       "      <td>English</td>\n",
       "      <td>Spanish (LA)</td>\n",
       "      <td>2014-11-06 15:39:16</td>\n",
       "      <td>2014-11-06 15:39:18</td>\n",
       "      <td>2014-11-06 23:18:08</td>\n",
       "      <td>2014-11-10 00:00:44</td>\n",
       "      <td>2014-11-10 12:17:09</td>\n",
       "      <td>2014-11-10 12:17:09</td>\n",
       "      <td>5.78</td>\n",
       "      <td>13</td>\n",
       "      <td>75.14</td>\n",
       "      <td>5</td>\n",
       "      <td>Sparklight Media</td>\n",
       "      <td>Communication Services</td>\n",
       "      <td>Media</td>\n",
       "      <td>Interactive Media &amp; Services</td>\n",
       "      <td>Interactive Media &amp; Services</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153803</th>\n",
       "      <td>214379</td>\n",
       "      <td>RMT</td>\n",
       "      <td>10419128</td>\n",
       "      <td>2016-01-25 16:00:00</td>\n",
       "      <td>2016-01-26 12:00:00</td>\n",
       "      <td>ProofReading</td>\n",
       "      <td>English</td>\n",
       "      <td>Catalan</td>\n",
       "      <td>2016-01-25 12:12:58</td>\n",
       "      <td>2016-01-25 12:57:40</td>\n",
       "      <td>2016-01-25 15:35:55</td>\n",
       "      <td>2016-01-25 16:12:16</td>\n",
       "      <td>2016-01-25 16:17:35</td>\n",
       "      <td>2016-01-25 16:17:35</td>\n",
       "      <td>0.29</td>\n",
       "      <td>15</td>\n",
       "      <td>4.35</td>\n",
       "      <td>5</td>\n",
       "      <td>TrueConnect</td>\n",
       "      <td>Communication Services</td>\n",
       "      <td>Interactive Media &amp; Services</td>\n",
       "      <td>Internet Services &amp; Infrastructure</td>\n",
       "      <td>Internet Services &amp; Infrastructure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283706</th>\n",
       "      <td>217532</td>\n",
       "      <td>KMT</td>\n",
       "      <td>10694449</td>\n",
       "      <td>2019-01-07 18:50:00</td>\n",
       "      <td>2019-01-07 21:00:00</td>\n",
       "      <td>Translation</td>\n",
       "      <td>English</td>\n",
       "      <td>Portuguese (Brazil)</td>\n",
       "      <td>2019-01-07 19:12:29</td>\n",
       "      <td>2019-01-07 19:12:57</td>\n",
       "      <td>2019-01-07 19:14:00</td>\n",
       "      <td>2019-01-07 19:16:03</td>\n",
       "      <td>2019-01-07 21:07:58</td>\n",
       "      <td>2019-01-07 21:08:04</td>\n",
       "      <td>0.03</td>\n",
       "      <td>15</td>\n",
       "      <td>0.45</td>\n",
       "      <td>10</td>\n",
       "      <td>SoftEcology</td>\n",
       "      <td>Information Technology</td>\n",
       "      <td>Software</td>\n",
       "      <td>Application Software</td>\n",
       "      <td>Environmental Software</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493944</th>\n",
       "      <td>221084</td>\n",
       "      <td>KMT</td>\n",
       "      <td>11109570</td>\n",
       "      <td>2022-02-24 15:58:00</td>\n",
       "      <td>2022-02-24 16:55:00</td>\n",
       "      <td>ProofReading</td>\n",
       "      <td>English</td>\n",
       "      <td>Spanish (LA)</td>\n",
       "      <td>2022-02-24 15:20:31</td>\n",
       "      <td>2022-02-24 15:57:26</td>\n",
       "      <td>2022-02-24 16:08:34</td>\n",
       "      <td>2022-02-24 16:20:36</td>\n",
       "      <td>2022-02-24 16:49:31</td>\n",
       "      <td>2022-02-24 16:55:10</td>\n",
       "      <td>0.00</td>\n",
       "      <td>14</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6</td>\n",
       "      <td>HealthyLife</td>\n",
       "      <td>Health Care</td>\n",
       "      <td>Health Care Providers</td>\n",
       "      <td>Health Care Facilities</td>\n",
       "      <td>Long-Term Care Facilities</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171666</th>\n",
       "      <td>214396</td>\n",
       "      <td>PMT</td>\n",
       "      <td>10455761</td>\n",
       "      <td>2016-07-04 15:34:00</td>\n",
       "      <td>2016-07-06 11:00:00</td>\n",
       "      <td>Translation</td>\n",
       "      <td>English</td>\n",
       "      <td>Spanish (Iberian)</td>\n",
       "      <td>2016-07-04 15:41:18</td>\n",
       "      <td>2016-07-04 15:43:01</td>\n",
       "      <td>2016-07-05 09:44:51</td>\n",
       "      <td>2016-07-06 11:04:49</td>\n",
       "      <td>2016-07-08 09:50:06</td>\n",
       "      <td>2016-07-08 09:50:06</td>\n",
       "      <td>4.33</td>\n",
       "      <td>16</td>\n",
       "      <td>69.28</td>\n",
       "      <td>5</td>\n",
       "      <td>WoodWorks</td>\n",
       "      <td>Consumer Discretionary</td>\n",
       "      <td>Leisure Products</td>\n",
       "      <td>Leisure Products</td>\n",
       "      <td>Leisure Products</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       PROJECT_ID   PM   TASK_ID               START                 END  \\\n",
       "106193     212576  PMT  10313113 2014-11-06 15:29:00 2014-11-10 09:00:00   \n",
       "153803     214379  RMT  10419128 2016-01-25 16:00:00 2016-01-26 12:00:00   \n",
       "283706     217532  KMT  10694449 2019-01-07 18:50:00 2019-01-07 21:00:00   \n",
       "493944     221084  KMT  11109570 2022-02-24 15:58:00 2022-02-24 16:55:00   \n",
       "171666     214396  PMT  10455761 2016-07-04 15:34:00 2016-07-06 11:00:00   \n",
       "\n",
       "           TASK_TYPE SOURCE_LANG          TARGET_LANG            ASSIGNED  \\\n",
       "106193   Translation     English         Spanish (LA) 2014-11-06 15:39:16   \n",
       "153803  ProofReading     English              Catalan 2016-01-25 12:12:58   \n",
       "283706   Translation     English  Portuguese (Brazil) 2019-01-07 19:12:29   \n",
       "493944  ProofReading     English         Spanish (LA) 2022-02-24 15:20:31   \n",
       "171666   Translation     English    Spanish (Iberian) 2016-07-04 15:41:18   \n",
       "\n",
       "                     READY             WORKING           DELIVERED  \\\n",
       "106193 2014-11-06 15:39:18 2014-11-06 23:18:08 2014-11-10 00:00:44   \n",
       "153803 2016-01-25 12:57:40 2016-01-25 15:35:55 2016-01-25 16:12:16   \n",
       "283706 2019-01-07 19:12:57 2019-01-07 19:14:00 2019-01-07 19:16:03   \n",
       "493944 2022-02-24 15:57:26 2022-02-24 16:08:34 2022-02-24 16:20:36   \n",
       "171666 2016-07-04 15:43:01 2016-07-05 09:44:51 2016-07-06 11:04:49   \n",
       "\n",
       "                  RECEIVED               CLOSE  FORECAST  HOURLY_RATE   COST  \\\n",
       "106193 2014-11-10 12:17:09 2014-11-10 12:17:09      5.78           13  75.14   \n",
       "153803 2016-01-25 16:17:35 2016-01-25 16:17:35      0.29           15   4.35   \n",
       "283706 2019-01-07 21:07:58 2019-01-07 21:08:04      0.03           15   0.45   \n",
       "493944 2022-02-24 16:49:31 2022-02-24 16:55:10      0.00           14   0.00   \n",
       "171666 2016-07-08 09:50:06 2016-07-08 09:50:06      4.33           16  69.28   \n",
       "\n",
       "        QUALITY_EVALUATION      MANUFACTURER     MANUFACTURER_SECTOR  \\\n",
       "106193                   5  Sparklight Media  Communication Services   \n",
       "153803                   5       TrueConnect  Communication Services   \n",
       "283706                  10       SoftEcology  Information Technology   \n",
       "493944                   6       HealthyLife             Health Care   \n",
       "171666                   5         WoodWorks  Consumer Discretionary   \n",
       "\n",
       "         MANUFACTURER_INDUSTRY_GROUP               MANUFACTURER_INDUSTRY  \\\n",
       "106193                         Media        Interactive Media & Services   \n",
       "153803  Interactive Media & Services  Internet Services & Infrastructure   \n",
       "283706                      Software                Application Software   \n",
       "493944         Health Care Providers              Health Care Facilities   \n",
       "171666              Leisure Products                    Leisure Products   \n",
       "\n",
       "                  MANUFACTURER_SUBINDUSTRY  \n",
       "106193        Interactive Media & Services  \n",
       "153803  Internet Services & Infrastructure  \n",
       "283706              Environmental Software  \n",
       "493944           Long-Term Care Facilities  \n",
       "171666                    Leisure Products  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation of what is happening\n",
    "\n",
    "In the global scope:\n",
    "\n",
    "0. We use as base of useful features the dataframe `transl_cost_pairs_df`\n",
    "It includes:\n",
    "- TRANSLATOR: Translator name.\n",
    "- SOURCE_LANG: Source language.\n",
    "- TARGET_LANG: Target language.\n",
    "- HOURLY_RATE: Cost per hour.\n",
    "\n",
    "\n",
    "1. We compute the **average proportional delay** (speed) from the `data_df` for each translator and merge it with this base dataframe\n",
    "2. We compute quality by languages\n",
    "2. We compute cuality by task type\n",
    "3. We compute experience and wxperience with client\n",
    "4. We compute availability\n",
    "\n",
    "\n",
    "Now for the each task:\n",
    "\n",
    "2. We start to the strict filter:\n",
    "    - Filter of **languages**: only consider the translators who offer this translation\n",
    "    - Filter of **price**: only consider the prices below the threshold\n",
    "    - Filter of **quality** by language: this is done by making an average of the quality of these languages for each translator, and then using it as a threshold. \n",
    "    - Filter of **availability**: if the shift lasts more than an hour from the start of the tasks or works for two consecutive days is considered available\n",
    "\n",
    "3. We do a **weighted knn**:\n",
    "    - We do it on the *perfect point* (price = 0, quality = 10, speed = 100%, experience = 10... (orientative values))\n",
    "    - The weights are chosen by the wildcards and by the expereince required for the type of task. A weighted knn, after normalizing, it distorts the chosen axis size to give more or less weight. \n",
    "    - The similarity score is the final ranking. \n",
    "\n",
    "4. Outcome possibilities: \n",
    "    - We get None or too few translators: if it is None we use the wildcard to completely ignore that factor in the strict filter\n",
    "    - We get a lot of translators: (we need the rest of features to give better recommendations)\n",
    "\n",
    "5. Calculate accuracy and find a way to retrieve explainations\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- THESE ARE GENERAL FUNCTIONS THAT CAN BE USED TO ACTUALIZE THE DATA IF NEW TASKS WERE ADDED TO THE DATASET -----\n",
    "def compute_delay_percentage(data_df, transl_cost_pairs_df):\n",
    "    \"\"\"\n",
    "    Compute the delay percentage of each translator based on task completion times.\n",
    "    Negative values indicate early delivery, positive values indicate late delivery.\n",
    "\n",
    "    Args:\n",
    "        data_df (pd.DataFrame): Task data with 'TRANSLATOR', 'START', 'END', 'DELIVERED'\n",
    "        transl_cost_pairs_df (pd.DataFrame): Translator costs with 'TRANSLATOR', 'COST'\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Merged dataframe with 'TRANSLATOR', 'COST', 'AVG_DELAY_PERCENTAGE'\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Convert date columns to datetime safely\n",
    "        date_cols = ['START', 'END', 'DELIVERED']\n",
    "        for col in date_cols:\n",
    "            data_df[col] = pd.to_datetime(data_df[col], errors='coerce')\n",
    "\n",
    "        # Calculate duration and filter out bad rows\n",
    "        duration = data_df['END'] - data_df['START']\n",
    "        invalid_rows = duration <= pd.Timedelta(0)\n",
    "        if invalid_rows.any():\n",
    "            print(f\"[INFO] Removed {invalid_rows.sum()} rows with zero or negative durations.\")\n",
    "            data_df = data_df[~invalid_rows]\n",
    "            duration = duration[~invalid_rows]\n",
    "\n",
    "        # Calculate delay percentage\n",
    "        delay = (data_df['DELIVERED'] - data_df['END']) / duration * 100\n",
    "        delay = delay.replace([np.inf, -np.inf, np.nan], 0).clip(-100, 100)\n",
    "        data_df = data_df.copy()\n",
    "        data_df['DELAY_PERCENTAGE'] = delay\n",
    "\n",
    "        # Compute average delay per translator\n",
    "        avg_delay = (\n",
    "            data_df.groupby('TRANSLATOR')['DELAY_PERCENTAGE']\n",
    "            .mean()\n",
    "            .round(2)\n",
    "            .reset_index()\n",
    "            .rename(columns={'DELAY_PERCENTAGE': 'AVG_DELAY_PERCENTAGE'})\n",
    "        )\n",
    "\n",
    "        # Merge with cost data\n",
    "        merged = transl_cost_pairs_df.merge(avg_delay, on='TRANSLATOR', how='left')\n",
    "        merged['AVG_DELAY_PERCENTAGE'] = merged['AVG_DELAY_PERCENTAGE'].fillna(0)\n",
    "\n",
    "        return merged\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed to compute delay percentages: {e}\")\n",
    "        return transl_cost_pairs_df.assign(AVG_DELAY_PERCENTAGE=0)\n",
    "\n",
    "\n",
    "def compute_number_tasks(data_df, translators_attributes_df):\n",
    "    \"\"\"\n",
    "    Computes the number of tasks for each translator.\n",
    "    \n",
    "    Args:\n",
    "        data_df (pd.DataFrame): \n",
    "            DataFrame containing the data of the tasks.\n",
    "        df_filtered (pd.DataFrame): \n",
    "            DataFrame containing the filtered translators' attributes.\n",
    "\n",
    "    Returns:\n",
    "        translators_attributes_df (pd.DataFrame) with the delay_percentage.\n",
    "    \"\"\"\n",
    "    # Count the number of tasks each translator has done\n",
    "    task_counts = data_df.groupby('TRANSLATOR').size().reset_index(name='NUM_TASKS')\n",
    "\n",
    "    # Merge the task counts into the filtered dataframe\n",
    "    translators_attributes_df = translators_attributes_df.merge(task_counts, on='TRANSLATOR', how='left')\n",
    "\n",
    "    # Fill missing values (i.e., translators with no tasks) with 0\n",
    "    translators_attributes_df['NUM_TASKS'] = translators_attributes_df['NUM_TASKS'].fillna(0).astype(int)\n",
    "\n",
    "    return translators_attributes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1742052911892,
     "user": {
      "displayName": "Rime Slaoui",
      "userId": "06907206809915164567"
     },
     "user_tz": -60
    },
    "id": "DdpXWG2XyfuP",
    "outputId": "704065d5-17d5-4fe8-fd7c-f45157aa07f6"
   },
   "outputs": [],
   "source": [
    "# ----- THESE ARE FUNCTIONS TO CALCULATE QUALITY AND EXPERIENCE (ONCE FILTERED BY LANGUAGES AND PRICE) -----\n",
    "def compute_quality_by_languages(data_df, df_filtered, source_lang, target_lang):\n",
    "    \"\"\"\n",
    "    Computes average quality for a given language pair (source_lang → target_lang).\n",
    "    If the translator has no experience with that task, falls back to:\n",
    "      - their overall average quality (with a penalty), or\n",
    "      - a 5 if no task has been done.\n",
    "    \n",
    "    Args:\n",
    "        df_filtered (pd.DataFrame): Filtered translators.\n",
    "        source_lang (str): Source language.\n",
    "        target_lang (str): Target language.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Same df_filtered with new 'AVG_QUALITY_BY_LG' column.\n",
    "    \"\"\"\n",
    "    if df_filtered.empty:\n",
    "        print(f\"Warning: No translators found in the filtered dataframe.\")\n",
    "        return df_filtered\n",
    "    \n",
    "    translators = df_filtered['TRANSLATOR'].unique()\n",
    "    \n",
    "    # Filter tasks dataframe by the language pair and translators in df_filtered\n",
    "    mask_lang_pair = (\n",
    "        (data_df['SOURCE_LANG'] == source_lang) &\n",
    "        (data_df['TARGET_LANG'] == target_lang) &\n",
    "        (data_df['TRANSLATOR'].isin(df_filtered['TRANSLATOR']))\n",
    "    )\n",
    "\n",
    "\n",
    "    # Compute the average quality for each translator in the filtered dataframe\n",
    "    avg_quality = (\n",
    "        data_df[mask_lang_pair]\n",
    "        .groupby('TRANSLATOR')['QUALITY_EVALUATION']\n",
    "        .mean()\n",
    "        .round(2)\n",
    "    )\n",
    "\n",
    "    # Assing the average quality to the filtered df\n",
    "    df_filtered['AVG_QUALITY_BY_LG'] = df_filtered['TRANSLATOR'].map(avg_quality)\n",
    "\n",
    "    # Fallback to penalized overall average\n",
    "    mask_missing = df_filtered['AVG_QUALITY_BY_LG'].isna()\n",
    "\n",
    "    overall_avg = (\n",
    "        data_df[data_df['TRANSLATOR'].isin(translators)]\n",
    "        .groupby('TRANSLATOR')['QUALITY_EVALUATION']\n",
    "        .mean()\n",
    "        .round(2)\n",
    "        .apply(lambda x: x - 1 if pd.notnull(x) else None)  # configurable penalization, for flexibility\n",
    "        #For a data-driven approach, use can standard deviation or percentile-based penalization to adapt to the distribution of quality scores\n",
    "    )\n",
    "\n",
    "    df_filtered.loc[mask_missing, 'AVG_QUALITY_BY_LG'] = df_filtered.loc[mask_missing, 'TRANSLATOR'].map(overall_avg)\n",
    "\n",
    "    # Para los traductores que no existen en data_df → asignar calidad por defecto (ej. 5)\n",
    "    df_filtered['AVG_QUALITY_BY_LG'] = df_filtered['AVG_QUALITY_BY_LG'].fillna(5)\n",
    "        \n",
    "    return df_filtered\n",
    "\n",
    "\n",
    "def compute_quality_by_task_type(data_df, df_filtered, task_type):\n",
    "    \"\"\"\n",
    "    Computes the average quality for each translator for a given task type.\n",
    "    If the translator has no experience with that task, falls back to:\n",
    "      - their overall average quality (with a penalty), or\n",
    "      - a 5 if no task has been done.\n",
    "    \n",
    "    Args:\n",
    "        df_filtered (pd.DataFrame): DataFrame with filtered translators.\n",
    "        task_type (str): The specific task type to evaluate.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: df_filtered with 'AVG_QUALITY_BY_TASK'.\n",
    "    \"\"\"\n",
    "    if df_filtered.empty:\n",
    "        print(f\"Warning: No translators found in the filtered dataframe.\")\n",
    "        return df_filtered\n",
    "    \n",
    "    translators = df_filtered['TRANSLATOR'].unique()\n",
    "\n",
    "    # 1. Compute average quality for given task type\n",
    "    mask_task = (\n",
    "        (data_df['TASK_TYPE'] == task_type) &\n",
    "        (data_df['TRANSLATOR'].isin(translators))\n",
    "    )\n",
    "\n",
    "    avg_by_task = (\n",
    "        data_df[mask_task]\n",
    "        .groupby('TRANSLATOR')['QUALITY_EVALUATION']\n",
    "        .mean()\n",
    "        .round(2)\n",
    "    )\n",
    "\n",
    "    df_filtered['AVG_QUALITY_BY_TASK'] = df_filtered['TRANSLATOR'].map(avg_by_task)\n",
    "\n",
    "    # 2. Fallback to penalized overall average\n",
    "    mask_missing = df_filtered['AVG_QUALITY_BY_TASK'].isna()\n",
    "\n",
    "    overall_avg = (\n",
    "        data_df[data_df['TRANSLATOR'].isin(translators)]\n",
    "        .groupby('TRANSLATOR')['QUALITY_EVALUATION']\n",
    "        .mean()\n",
    "        .round(2)\n",
    "        .apply(lambda x: x - 1 if pd.notnull(x) else None)  # configurable penalization, for flexibility\n",
    "        #For a data-driven approach, use can standard deviation or percentile-based penalization to adapt to the distribution of quality scores\n",
    "    )\n",
    "\n",
    "    df_filtered.loc[mask_missing, 'AVG_QUALITY_BY_TASK'] = df_filtered.loc[mask_missing, 'TRANSLATOR'].map(overall_avg)\n",
    "\n",
    "    df_filtered['AVG_QUALITY_BY_TASK'] = df_filtered['AVG_QUALITY_BY_TASK'].fillna(5)\n",
    "\n",
    "    return df_filtered\n",
    "\n",
    "\n",
    "def compute_experience(df_filtered, data_df, task_type, source_lang, target_lang, industry, subindustry):\n",
    "    \"\"\"\n",
    "    Computes a soft experience score for each translator based on how many\n",
    "    dimensions match (task_type, language pair, industry, subindustry).\n",
    "\n",
    "    Args:\n",
    "        df_filtered (pd.DataFrame): Filtered translators' dataframe.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: With added column 'EXPERIENCE_SCORE'.\n",
    "    \"\"\"\n",
    "    TASK_TYPE_BONUS = {\n",
    "        'LanguageLead': 0.5,\n",
    "        'ProofReading': 0.5,\n",
    "        'Spotcheck': 0.5\n",
    "    }\n",
    "\n",
    "    translators = df_filtered['TRANSLATOR'].unique()\n",
    "\n",
    "    df = data_df[data_df['TRANSLATOR'].isin(translators)].copy()\n",
    "\n",
    "    # Base score: match on source, target, task_type\n",
    "    df['score'] = 0\n",
    "    df['score'] += (df['SOURCE_LANG'] == source_lang).astype(int)\n",
    "    df['score'] += (df['TARGET_LANG'] == target_lang).astype(int)\n",
    "    df['score'] += (df['TASK_TYPE'] == task_type).astype(int)\n",
    "\n",
    "    # Only add 1 point if industry or subindustry match\n",
    "    industry_match = (df['MANUFACTURER_INDUSTRY'] == industry)\n",
    "    subindustry_match = (df['MANUFACTURER_SUBINDUSTRY'] == subindustry)\n",
    "    df['score'] += ((industry_match | subindustry_match)).astype(int)\n",
    "\n",
    "    # Advanced task bonus\n",
    "    bonus_df = df[df['TASK_TYPE'].isin(TASK_TYPE_BONUS)].copy()\n",
    "    bonus_df['bonus'] = bonus_df['TASK_TYPE'].map(TASK_TYPE_BONUS)\n",
    "    bonus_scores = bonus_df.groupby('TRANSLATOR')['bonus'].sum()\n",
    "\n",
    "    # Base score\n",
    "    base_scores = df.groupby('TRANSLATOR')['score'].sum()\n",
    "\n",
    "    # Total experience = base + bonus\n",
    "    total_score = base_scores.add(bonus_scores, fill_value=0)\n",
    "\n",
    "    df_filtered['EXPERIENCE_SCORE'] = df_filtered['TRANSLATOR'].map(total_score).fillna(0)\n",
    "\n",
    "    # Normalize between 0 and 10\n",
    "    min_score = df_filtered['EXPERIENCE_SCORE'].min()\n",
    "    max_score = df_filtered['EXPERIENCE_SCORE'].max()\n",
    "\n",
    "    if max_score > min_score:\n",
    "        df_filtered['EXPERIENCE_SCORE'] = (\n",
    "            (df_filtered['EXPERIENCE_SCORE'] - min_score) / (max_score - min_score)\n",
    "        ) * 10\n",
    "    else:\n",
    "        df_filtered['EXPERIENCE_SCORE'] = 0\n",
    "\n",
    "    df_filtered['EXPERIENCE_SCORE'] = df_filtered['EXPERIENCE_SCORE'].round(2)\n",
    "\n",
    "    # Detect translators not present in data_df (no prior tasks)\n",
    "    missing_translators_mask = ~df_filtered['TRANSLATOR'].isin(data_df['TRANSLATOR'])\n",
    "\n",
    "    # if not missing_translators.empty:\n",
    "    #     print(\"Translators with no experience data:\")\n",
    "    #     print(missing_translators.tolist())\n",
    "\n",
    "    # Compute average from those with scores\n",
    "    avg_experience = df_filtered.loc[~missing_translators_mask, 'EXPERIENCE_SCORE'].mean()\n",
    "    # print(f\"Assigning average experience score of {round(avg_experience, 2)} to missing translators.\")\n",
    "\n",
    "    # Assign average to missing\n",
    "    df_filtered.loc[missing_translators_mask, 'EXPERIENCE_SCORE'] = avg_experience\n",
    "\n",
    "    return df_filtered\n",
    "\n",
    "\n",
    "def compute_experience_for_client(df_filtered, data_df, client):\n",
    "    \"\"\"\n",
    "    Computes an experience score for each translator based on a specific client\n",
    "\n",
    "    Args:\n",
    "        df_filtered (pd.DataFrame): Filtered translators' dataframe.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: With added column 'EXPERIENCE_CLIENT'.\n",
    "    \"\"\"\n",
    "    translators = df_filtered['TRANSLATOR'].unique()\n",
    "    df = data_df[data_df['TRANSLATOR'].isin(translators)].copy()\n",
    "\n",
    "\n",
    "    df['score'] = 0\n",
    "    df['score'] += (df['MANUFACTURER'] == client).astype(int)\n",
    "\n",
    "    # Total experience score = sum of weights per translator\n",
    "    experience_scores = df.groupby('TRANSLATOR')['score'].sum()\n",
    "\n",
    "    # Add to filtered dataframe\n",
    "    df_filtered['EXPERIENCE_CLIENT'] = df_filtered['TRANSLATOR'].map(experience_scores).fillna(0).astype(int)\n",
    "\n",
    "    # Normalizar entre 0 y 10\n",
    "    min_score = df_filtered['EXPERIENCE_CLIENT'].min()\n",
    "    max_score = df_filtered['EXPERIENCE_CLIENT'].max()\n",
    "\n",
    "    if max_score > min_score:  # Evitar división por 0\n",
    "        df_filtered['EXPERIENCE_CLIENT'] = ((df_filtered['EXPERIENCE_CLIENT'] - min_score) / (max_score - min_score)) * 10\n",
    "    else:\n",
    "        df_filtered['EXPERIENCE_CLIENT'] = 0  # Si todos los scores son iguales\n",
    "\n",
    "    df_filtered['EXPERIENCE_CLIENT'] = df_filtered['EXPERIENCE_CLIENT'].round(2)\n",
    "\n",
    "    return df_filtered\n",
    "\n",
    "\n",
    "def available_translators(task, translators_attributes_df, schedules_df, TRANSLATORS_UNAVAILABLE):\n",
    "    \"\"\"\n",
    "    Checks if translators are available for the task based on their weekly working schedule.\n",
    "    This, for now just takes into account the day of the week and the start time of the task. \n",
    "    TAKE INTO ACCOUNT: This can have problems if the translator is at the end of their weekly shedule, it also doesnt take into account multitasking.\n",
    "    \n",
    "    Args:\n",
    "        task (Task object): The task for which we want to check availability.\n",
    "        translators_attributes_df (pd.DataFrame): DataFrame containing the translators' attributes.\n",
    "        schedules_df (pd.DataFrame): DataFrame containing the weekly schedules of translators.\n",
    "        TRANSLATORS_UNAVAILABLE (list): List of translators who are unavailable.\n",
    "        \n",
    "    Returns:\n",
    "        df_filtered (pd.DataFrame): Filtered DataFrame containing translators who are available.\n",
    "    \"\"\"\n",
    "    # 1. Remove explicitly unavailable translators\n",
    "    df_filtered = translators_attributes_df[~translators_attributes_df['TRANSLATOR'].isin(TRANSLATORS_UNAVAILABLE)].copy()\n",
    "\n",
    "    # 2. Extract day of week and time from task\n",
    "    task_day = task.ASSIGNED.strftime('%a').upper()  #day of the week  e.g., 'MON', 'TUE'\n",
    "\n",
    "    # 3. Merge schedule info\n",
    "    df_filtered = df_filtered.merge(schedules_df, left_on='TRANSLATOR', right_on='NAME').drop(columns=['NAME'])\n",
    "\n",
    "    def is_available(row):\n",
    "        # 1. Verificar si trabaja ese día\n",
    "        if row[task_day] != 1:\n",
    "            return False\n",
    "\n",
    "        # 2. Tiempos\n",
    "        task_start_time = timedelta(hours=task.ASSIGNED.hour, minutes=task.ASSIGNED.minute)\n",
    "        work_end_time = timedelta(hours=row['END'].hour, minutes=row['END'].minute)\n",
    "\n",
    "        # 3. Caso 1: el turno dura al menos una hora desde el inicio de la tarea\n",
    "        if work_end_time - task_start_time >= timedelta(hours=1):\n",
    "            return True\n",
    "\n",
    "        # 4. Caso 2: turno corto, pero trabaja mañana\n",
    "        days = ['MON', 'TUE', 'WED', 'THU', 'FRI', 'SAT', 'SUN']\n",
    "        next_day = days[(days.index(task_day) + 1) % 7]\n",
    "        return row[next_day] == 1\n",
    "\n",
    "    # 4. Apply availability logic\n",
    "    df_filtered['IS_AVAILABLE'] = df_filtered.apply(is_available, axis=1)\n",
    "    df_filtered = df_filtered[df_filtered['IS_AVAILABLE'] == True].drop(columns=['IS_AVAILABLE'])\n",
    "\n",
    "    return df_filtered\n",
    "\n",
    "\n",
    "# ----- PRINCIPAL FUNCTION TO FILTER THE TRANSLATORS' ATTRIBUTES -----\n",
    "def filter_language_price_quality_availability(data_df, schedules_df, translators_attributes_df, task = Task, need_wildcard = False):\n",
    "    \"\"\"\n",
    "    Filters the translators' attributes by languages, price, quality and availability.\n",
    "    If need_wildcard is True, it will skip the filter corresponding to the wildcard.\n",
    "\n",
    "    Structured fallback:\n",
    "        Tries a strict filter.\n",
    "        If that fails, it retries with a wildcard (skipping one constraint).\n",
    "        If that also fails, it relaxes all filters except language pair. That's reasonable.\n",
    "    \n",
    "    Args:\n",
    "        translators_attributes_df (pd.DataFrame): \n",
    "            DataFrame containing the translators' attributes (name, languages, price, speed).\n",
    "        task (Task object): \n",
    "            The task for which we want to filter the translators.\n",
    "        need_wildcard (bool): \n",
    "            If True, skip the filter corresponding to the wildcard.\n",
    "            \n",
    "    Returns:\n",
    "        pd.DataFrame: \n",
    "            Filtered DataFrame containing translators who meet the criteria.\n",
    "    \"\"\"\n",
    "    \n",
    "    if not need_wildcard:\n",
    "        # Filter by language, price HARD FILTER\n",
    "        df_filtered = translators_attributes_df[\n",
    "            (translators_attributes_df['SOURCE_LANG'] == task.SOURCE_LANG) & \n",
    "            (translators_attributes_df['TARGET_LANG'] == task.TARGET_LANG) &\n",
    "            (translators_attributes_df['HOURLY_RATE'] <= task.SELLING_HOURLY_PRICE) \n",
    "        ].copy()\n",
    "\n",
    "        # If the filtered dataframe is empty, it's because the budget is too low, therefore we have to relax the filter\n",
    "        if df_filtered.empty:\n",
    "            if task.WILDCARD == \"Price\":\n",
    "                print(f\"Warning: No translators found for task {task.TASK_ID}, wildcard= {task.WILDCARD} because the BUDGET is too low. Trying with wildcard...\")\n",
    "                return filter_language_price_quality_availability(data_df, schedules_df, translators_attributes_df, task = task, need_wildcard = True)\n",
    "            else: \n",
    "                print(f\"Warning: No translators found for task {task.TASK_ID}, wildcard= {task.WILDCARD} because the BUDGET is too low. Relaxing price filter...\")\n",
    "                # Skip the price filter and try again\n",
    "                df_filtered = translators_attributes_df[\n",
    "                    (translators_attributes_df['SOURCE_LANG'] == task.SOURCE_LANG) &\n",
    "                    (translators_attributes_df['TARGET_LANG'] == task.TARGET_LANG)\n",
    "                ].copy()\n",
    "\n",
    "        # add the average quality column\n",
    "        df_filtered = compute_quality_by_task_type(data_df, df_filtered, task_type=task.TASK_TYPE)\n",
    "        df_filtered = compute_quality_by_languages(data_df, df_filtered, source_lang=task.SOURCE_LANG, target_lang=task.TARGET_LANG)\n",
    "\n",
    "        df_filtered_quality = df_filtered[df_filtered['AVG_QUALITY_BY_LG'] >= task.MIN_QUALITY]\n",
    "\n",
    "        # If the filtered dataframe is empty, it's because the quality is too high.\n",
    "        if df_filtered_quality.empty:\n",
    "            if task.WILDCARD == \"Quality\":\n",
    "                print(f\"Warning: No translators found for task {task.TASK_ID}, wildcard= {task.WILDCARD} because the QUALITY is too high. Trying with wildcard...\")\n",
    "                return filter_language_price_quality_availability(data_df, schedules_df, translators_attributes_df, task = task, need_wildcard = True)\n",
    "            else: \n",
    "                print(f\"Warning: No translators found for task {task.TASK_ID}, wildcard= {task.WILDCARD} because the QUALITY is too high. Relaxing quality filter...\")\n",
    "                # Skip the quality filter and try again\n",
    "                df_filtered_quality = df_filtered[df_filtered['AVG_QUALITY_BY_LG'] >= task.MIN_QUALITY-2] # Relaxing the quality filter by 2 points\n",
    "            \n",
    "        \n",
    "        # Filter by availability\n",
    "        df_filtered_availability = available_translators(task, df_filtered_quality, schedules_df, TRANSLATORS_UNAVAILABLE)\n",
    "\n",
    "        if df_filtered_availability.empty:\n",
    "            if task.WILDCARD == \"Deadline\":\n",
    "                print(f\"Warning: No translators found for task {task.TASK_ID}, wildcard= {task.WILDCARD} because TRANSLATORS ARE UNAVAILABLE. Trying with wildcard...\")\n",
    "                return filter_language_price_quality_availability(data_df, schedules_df, translators_attributes_df, task = task, need_wildcard = True)\n",
    "            else: \n",
    "                print(f\"Warning: No translators found for task {task.TASK_ID}, wildcard= {task.WILDCARD} because TRANSLATORS ARE UNAVAILABLE. Relaxing availability filter...\")\n",
    "                return df_filtered_quality\n",
    "\n",
    "        return df_filtered_availability\n",
    "    \n",
    "    # same code as above but with the wildcard, it will skip the filter corresponding to the wildcard\n",
    "    else:\n",
    "        # if the wildcard is \"Price\", we don't filter by price\n",
    "        price_condition = (translators_attributes_df['HOURLY_RATE'] <= task.SELLING_HOURLY_PRICE) if task.WILDCARD != \"Price\" else True\n",
    "        # Filter by language, price \n",
    "        df_filtered = translators_attributes_df[\n",
    "            (translators_attributes_df['SOURCE_LANG'] == task.SOURCE_LANG) & \n",
    "            (translators_attributes_df['TARGET_LANG'] == task.TARGET_LANG) &\n",
    "            price_condition \n",
    "        ].copy()\n",
    "\n",
    "        if df_filtered.empty:\n",
    "            print(f\"Warning: No translators found for task {task.TASK_ID}, wildcard= {task.WILDCARD} because the BUDGET is too low. Relaxing price filter...\")\n",
    "            # Skip the price filter and try again\n",
    "            df_filtered = translators_attributes_df[\n",
    "                (translators_attributes_df['SOURCE_LANG'] == task.SOURCE_LANG) &\n",
    "                (translators_attributes_df['TARGET_LANG'] == task.TARGET_LANG)\n",
    "            ].copy()\n",
    "\n",
    "        # add the average quality column\n",
    "        df_filtered = compute_quality_by_languages(data_df, df_filtered, source_lang=task.SOURCE_LANG, target_lang=task.TARGET_LANG)\n",
    "        df_filtered = compute_quality_by_task_type(data_df, df_filtered, task_type=task.TASK_TYPE)\n",
    "\n",
    "        if task.WILDCARD != \"Quality\":\n",
    "            df_filtered_quality = df_filtered[df_filtered['AVG_QUALITY_BY_LG'] >= task.MIN_QUALITY]\n",
    "\n",
    "            if df_filtered_quality.empty:\n",
    "                print(f\"Warning: No translators found for task {task.TASK_ID}, wildcard= {task.WILDCARD} because the QUALITY is too high. Relaxing quality filter...\")\n",
    "                # Skip the quality filter and try again\n",
    "                df_filtered_quality = df_filtered[df_filtered['AVG_QUALITY_BY_LG'] >= task.MIN_QUALITY-2] # Relaxing the quality filter by 2 points\n",
    "        else:\n",
    "            df_filtered_quality = df_filtered.copy()\n",
    "\n",
    "        if task.WILDCARD != \"Deadline\":  \n",
    "            # Filter by availability\n",
    "            df_filtered_availability = available_translators(task, df_filtered_quality, schedules_df, TRANSLATORS_UNAVAILABLE)\n",
    "\n",
    "            if df_filtered_availability.empty:\n",
    "                print(f\"Warning: No translators found for task {task.TASK_ID}, wildcard= {task.WILDCARD} because TRANSLATORS ARE UNAVAILABLE. Relaxing availability filter...\")\n",
    "                return df_filtered_quality\n",
    "        \n",
    "        return df_filtered\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nAt this stage, we have a filtered dataframe with the translators that are available for the task,\\nit contains the following columns:\\n    - TRANSLATOR: Name of the translator\\n    - SOURCE_LANG: Source language of the translator\\n    - TARGET_LANG: Target language of the translator\\n    - HOURLY_RATE: Hourly rate of the translator\\n    - Filtered by the availability\\n\\n    Things to take into account for the calculation of the scores:\\n    - AVG_QUALITY_BY_LNG: Average quality by language pair (if applicable)\\n    - AVG_QUALITY_BY_TASK: Average quality by task type (if applicable)\\n    - AVG_DELAY_PERCENTAGE: Average delay percentage of the translator (if applicable)\\n\\n    - NUM_TASKS: Number of tasks performed by the translator (to take into account the reliability of the translator's quality and delay percentage, not experience because it is calculated based on specific tasks)\\n    - EXPERIENCE_SCORE: Experience score based on task type, language pair, industry, and subindustry\\n    - EXPERIENCE_CLIENT: Experience score based on the specific client (if applicable)\\n\\n    \\nThere are some key considerations regarding the experience and quality weights:\\n    - Proofreading and Spotcheck need more expereinced translators.\\n    - LanguageLead is a more advanced task, so it needs more experience and quality.\\n    - Test should be assigned to the most experienced and high-quality TRANSLATOR for the client or topic, regardless of price.\\n    - For training we dont need to take into account the experience nor the quality\\n\\n\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "At this stage, we have a filtered dataframe with the translators that are available for the task,\n",
    "it contains the following columns:\n",
    "    - TRANSLATOR: Name of the translator\n",
    "    - SOURCE_LANG: Source language of the translator\n",
    "    - TARGET_LANG: Target language of the translator\n",
    "    - HOURLY_RATE: Hourly rate of the translator\n",
    "    - Filtered by the availability\n",
    "\n",
    "    Things to take into account for the calculation of the scores:\n",
    "    - AVG_QUALITY_BY_LNG: Average quality by language pair (if applicable)\n",
    "    - AVG_QUALITY_BY_TASK: Average quality by task type (if applicable)\n",
    "    - AVG_DELAY_PERCENTAGE: Average delay percentage of the translator (if applicable)\n",
    "\n",
    "    - NUM_TASKS: Number of tasks performed by the translator (to take into account the reliability of the translator's quality and delay percentage, not experience because it is calculated based on specific tasks)\n",
    "    - EXPERIENCE_SCORE: Experience score based on task type, language pair, industry, and subindustry\n",
    "    - EXPERIENCE_CLIENT: Experience score based on the specific client (if applicable)\n",
    "\n",
    "    \n",
    "There are some key considerations regarding the experience and quality weights:\n",
    "    - Proofreading and Spotcheck need more expereinced translators.\n",
    "    - LanguageLead is a more advanced task, so it needs more experience and quality.\n",
    "    - Test should be assigned to the most experienced and high-quality TRANSLATOR for the client or topic, regardless of price.\n",
    "    - For training we dont need to take into account the experience nor the quality\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_past_tasks_features(data_df, task):\n",
    "    \"\"\"\n",
    "    Filters past tasks similar to the given task and computes relevant features for scoring translators.\n",
    "\n",
    "    Parameters:\n",
    "        data_df (pd.DataFrame): Full dataset of past translation tasks.\n",
    "        task (object): The task object containing attributes like TASK_TYPE, SOURCE_LANG, etc.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A filtered and enriched DataFrame with computed features and filled missing values.\n",
    "    \"\"\"\n",
    "    df = data_df[\n",
    "        (data_df['TASK_TYPE'] == task.TASK_TYPE) &\n",
    "        (data_df['SOURCE_LANG'] == task.SOURCE_LANG) & \n",
    "        (data_df['TARGET_LANG'] == task.TARGET_LANG)\n",
    "    ].copy()  # <--- importante hacer copy()\n",
    "\n",
    "    df = compute_quality_by_languages(data_df, df, source_lang=task.SOURCE_LANG, target_lang=task.TARGET_LANG)\n",
    "    df = compute_quality_by_task_type(data_df, df, task_type=task.TASK_TYPE)\n",
    "    df = compute_experience(df, data_df, task.TASK_TYPE, task.SOURCE_LANG, task.TARGET_LANG, task.MANUFACTURER_INDUSTRY, task.MANUFACTURER_SUBINDUSTRY)\n",
    "    df = compute_experience_for_client(df, data_df, task.MANUFACTURER)\n",
    "    df = compute_delay_percentage(data_df, df)\n",
    "\n",
    "    # Rellenar NaNs generados en las nuevas columnas con valores razonables (ejemplo):\n",
    "    fill_defaults = {\n",
    "        'AVG_QUALITY_BY_LG': 5,\n",
    "        'AVG_QUALITY_BY_TASK': 5,\n",
    "        'EXPERIENCE_SCORE': 0,\n",
    "        'EXPERIENCE_CLIENT': 0,\n",
    "        'AVG_DELAY_PERCENTAGE': 0,\n",
    "        'HOURLY_RATE': df['HOURLY_RATE'].median() if 'HOURLY_RATE' in df else 0,\n",
    "    }\n",
    "    for col, val in fill_defaults.items():\n",
    "        if col in df.columns:\n",
    "            df.loc[:, col] = df[col].fillna(val)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def train_feature_weight_model(df_past_tasks, target='QUALITY_EVALUATION'):\n",
    "    \"\"\"\n",
    "    Trains a linear regression model to learn the importance (weights) of different features \n",
    "    in predicting a target such as quality.\n",
    "\n",
    "    Parameters:\n",
    "        df_past_tasks (pd.DataFrame): DataFrame with past task features and quality evaluations.\n",
    "        target (str): The name of the target column (default is 'QUALITY_EVALUATION').\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: A normalized weight vector (1D array) indicating the importance of each feature.\n",
    "    \"\"\"\n",
    "    categorical_cols = ['MANUFACTURER', 'MANUFACTURER_INDUSTRY', 'MANUFACTURER_SUBINDUSTRY']\n",
    "\n",
    "    numeric_cols = ['HOURLY_RATE', 'AVG_QUALITY_BY_LG', 'AVG_QUALITY_BY_TASK',\n",
    "                'AVG_DELAY_PERCENTAGE', 'EXPERIENCE_SCORE', 'EXPERIENCE_CLIENT']\n",
    "\n",
    "    df_past_tasks = df_past_tasks.copy()\n",
    "\n",
    "    # Rellenar NaNs en features numéricas\n",
    "    fill_defaults = {\n",
    "        'AVG_QUALITY_BY_LG': 5,\n",
    "        'AVG_QUALITY_BY_TASK': 5,\n",
    "        'EXPERIENCE_SCORE': 0,\n",
    "        'EXPERIENCE_CLIENT': 0,\n",
    "        'AVG_DELAY_PERCENTAGE': 0,\n",
    "        'HOURLY_RATE': df_past_tasks['HOURLY_RATE'].median() if 'HOURLY_RATE' in df_past_tasks else 0,\n",
    "    }\n",
    "    for col, val in fill_defaults.items():\n",
    "        if col in df_past_tasks.columns:\n",
    "            df_past_tasks.loc[:, col] = df_past_tasks[col].fillna(val)\n",
    "\n",
    "    X = df_past_tasks[categorical_cols + numeric_cols]\n",
    "    y = df_past_tasks[target]\n",
    "\n",
    "    preprocessor = ColumnTransformer([\n",
    "        ('cat', OneHotEncoder(drop='first'), categorical_cols)\n",
    "    ], remainder='passthrough')\n",
    "\n",
    "    model = Pipeline([\n",
    "        ('pre', preprocessor),\n",
    "        ('reg', LinearRegression())\n",
    "    ])\n",
    "\n",
    "    model.fit(X, y)\n",
    "\n",
    "    coef = model.named_steps['reg'].coef_\n",
    "    feature_names = model.named_steps['pre'].get_feature_names_out()\n",
    "\n",
    "    feature_weights = {\n",
    "        name: abs(weight)\n",
    "        for name, weight in zip(feature_names, coef)\n",
    "        if name in numeric_cols\n",
    "    }\n",
    "\n",
    "    weights_vector = np.array([feature_weights.get(f, 0.0) for f in numeric_cols])\n",
    "    s = weights_vector.sum()\n",
    "    if s == 0:\n",
    "        weights_vector = np.ones_like(weights_vector) / len(weights_vector)\n",
    "    else:\n",
    "        weights_vector = weights_vector / s\n",
    "\n",
    "    return weights_vector\n",
    "\n",
    "\n",
    "def get_dynamic_ideal(df_past_tasks, features):\n",
    "    \"\"\"\n",
    "    Computes the ideal feature vector for a given task using the top 10 highest quality past translations \n",
    "    for the same task type and language pair.\n",
    "\n",
    "    Parameters:\n",
    "        df_past_tasks (pd.DataFrame): DataFrame with all past tasks.\n",
    "        features (list): List of feature names to include in the ideal vector.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Mean feature vector representing an \"ideal\" translator for the given task.\n",
    "    \"\"\"\n",
    "    top_translators = df_past_tasks.sort_values(by='QUALITY_EVALUATION', ascending=False).head(10)\n",
    "\n",
    "    ideal_vector = top_translators[features].mean().values\n",
    "    return ideal_vector\n",
    "\n",
    "\n",
    "def knn(df_filtered, data_df, task, metric='euclidean', need_wildcard=False):\n",
    "    \"\"\"\n",
    "    Finds the nearest translators to the dynamically computed ideal translator using weighted KNN.\n",
    "\n",
    "    Parameters:\n",
    "        df_filtered (pd.DataFrame): Candidate translators already filtered by basic criteria.\n",
    "        task (object): The new task for which we want to find the best translator.\n",
    "        df_past_tasks (pd.DataFrame): All past tasks used for computing the dynamic ideal.\n",
    "        weight_vector (np.ndarray): Feature importance weights learned from historical data.\n",
    "        metric (str): Distance metric to use in KNN (default is 'euclidean').\n",
    "        need_wildcard (bool): If True, skip wildcard weighting adjustment.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (distances, indexes) from KNN representing closeness to the ideal translator.\n",
    "    \"\"\"\n",
    "    features = ['HOURLY_RATE', 'AVG_QUALITY_BY_LG', 'AVG_QUALITY_BY_TASK',\n",
    "                'AVG_DELAY_PERCENTAGE', 'EXPERIENCE_SCORE', 'EXPERIENCE_CLIENT']\n",
    "\n",
    "    # 1. Pesos dinámicos aprendidos desde el histórico\n",
    "    past_data = prepare_past_tasks_features(data_df, task)\n",
    "    #weights = np.array([1, 1.5, 1.5, 0.25, 1, 0.5])  # Default weights for the features\n",
    "    weights = train_feature_weight_model(past_data)\n",
    "\n",
    "    # 3. Vector ideal dinámico\n",
    "    ideal_values = get_dynamic_ideal(past_data, features)\n",
    "\n",
    "    # 4. Preparar datos para KNN\n",
    "    X = df_filtered[features]\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    X_weighted = X_scaled * weights\n",
    "\n",
    "    knn = NearestNeighbors(metric=metric)\n",
    "    knn.fit(X_weighted)\n",
    "\n",
    "    task_df = pd.DataFrame([ideal_values], columns=features)\n",
    "    task_scaled = scaler.transform(task_df)\n",
    "    task_weighted = task_scaled * weights\n",
    "\n",
    "    distances, indexes = knn.kneighbors(task_weighted, n_neighbors=len(df_filtered))\n",
    "\n",
    "    return distances, indexes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_translators(df_filtered, indexes, distances):\n",
    "    \"\"\"\n",
    "    Get the best translators based on the KNN results.\n",
    "    \n",
    "    Args:\n",
    "        df_filtered (pd.DataFrame): \n",
    "            Contains the filtered translators' attributes (name, language, price, quality, speed).\n",
    "        indexes (np.ndarray): \n",
    "            Indices of the nearest neighbors in the df_filtered.\n",
    "        distances (np.ndarray): \n",
    "            Distances of the nearest neighbors.\n",
    "            \n",
    "    Returns:\n",
    "        df_filtered (pd.DataFrame): \n",
    "            Contains the filtered translators' attributes (name, language, price, quality, speed AND similarity_score).\n",
    "    \"\"\"\n",
    "    \n",
    "    selected_translators = df_filtered.iloc[indexes[0]].copy()\n",
    "    \n",
    "    # Add the similarity score\n",
    "    selected_translators['Similarity Score'] = distances[0].round(2)  # Round to 2 decimal places\n",
    "\n",
    "    # Sort by similarity score (ascending: closest match first)\n",
    "    selected_translators = selected_translators.sort_values(by='Similarity Score', ascending=False) \n",
    "\n",
    "    return selected_translators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_metrics(df_filtered, task):\n",
    "    metrics = ['euclidean', 'manhattan']\n",
    "    results = {}\n",
    "\n",
    "    for metric in metrics:\n",
    "        distances, indexes = knn(df_filtered, task, metric=metric, need_wildcard=False)\n",
    "        selected_translators = get_best_translators(df_filtered, indexes, distances)\n",
    "        \n",
    "        results[metric] = selected_translators\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def plot_similarity(results):\n",
    "    \"\"\"\n",
    "    Plot the similarity scores for translators selected using different distance metrics.\n",
    "    \n",
    "    Args:\n",
    "        results (dict): A dictionary where keys are metric names and values are DataFrames \n",
    "                        with the selected translators' similarity scores.\n",
    "    \"\"\"\n",
    "    # Create a DataFrame to hold all similarity scores\n",
    "    similarity_data = []\n",
    "    for metric, translators in results.items():\n",
    "        # Add a new column with the metric name\n",
    "        translators['Metric'] = metric\n",
    "        similarity_data.append(translators)\n",
    "\n",
    "    # Concatenate all DataFrames\n",
    "    all_similarity_scores = pd.concat(similarity_data)\n",
    "\n",
    "    # Plot using seaborn\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.boxplot(x='Metric', y='Similarity Score', data=all_similarity_scores)\n",
    "    plt.title('Comparison of Similarity Scores for Different Metrics')\n",
    "    plt.xlabel('Distance Metric')\n",
    "    plt.ylabel('Similarity Score')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cosine similaruty is not considered, as it measures how similarly two vectors point in the same direction, regardless of their magnitude, and is used when relative proportions matter more than absolute values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No translators found for task 10370526, wildcard= Quality because the BUDGET is too low. Relaxing price filter...\n",
      "Warning: No translators found for task 11214145, wildcard= Price because the BUDGET is too low. Trying with wildcard...\n",
      "Warning: No translators found for task 10873457, wildcard= Quality because the BUDGET is too low. Relaxing price filter...\n",
      "Warning: No translators found for task 10428048, wildcard= Quality because the BUDGET is too low. Relaxing price filter...\n",
      "Warning: No translators found for task 11217498, wildcard= Deadline because the BUDGET is too low. Relaxing price filter...\n",
      "Warning: No translators found for task 11025093, wildcard= Deadline because the BUDGET is too low. Relaxing price filter...\n",
      "Warning: No translators found for task 10686839, wildcard= Quality because the BUDGET is too low. Relaxing price filter...\n",
      "Warning: No translators found for task 10738186, wildcard= Deadline because the BUDGET is too low. Relaxing price filter...\n",
      "Warning: No translators found for task 10149735, wildcard= Price because the QUALITY is too high. Relaxing quality filter...\n",
      "Warning: No translators found for task 10432570, wildcard= Deadline because the QUALITY is too high. Relaxing quality filter...\n",
      "Warning: No translators found for task 10302211, wildcard= Deadline because the QUALITY is too high. Relaxing quality filter...\n",
      "Warning: No translators found for task 10820508, wildcard= Deadline because the BUDGET is too low. Relaxing price filter...\n",
      "Warning: No translators found for task 11148957, wildcard= Deadline because the BUDGET is too low. Relaxing price filter...\n",
      "Warning: No translators found for task 10629046, wildcard= Deadline because the QUALITY is too high. Relaxing quality filter...\n",
      "Warning: No translators found for task 10117196, wildcard= Quality because the BUDGET is too low. Relaxing price filter...\n",
      "Warning: No translators found for task 10562192, wildcard= Price because the BUDGET is too low. Trying with wildcard...\n",
      "Warning: No translators found for task 10643439, wildcard= Deadline because the QUALITY is too high. Relaxing quality filter...\n",
      "Warning: No translators found for task 10581448, wildcard= Price because the BUDGET is too low. Trying with wildcard...\n",
      "Warning: No translators found for task 10795907, wildcard= Deadline because TRANSLATORS ARE UNAVAILABLE. Trying with wildcard...\n",
      "Warning: No translators found for task 10951177, wildcard= Price because the BUDGET is too low. Trying with wildcard...\n",
      "Warning: No translators found for task 10257793, wildcard= Deadline because the BUDGET is too low. Relaxing price filter...\n",
      "Warning: No translators found for task 10907777, wildcard= Deadline because the BUDGET is too low. Relaxing price filter...\n",
      "Warning: No translators found for task 11041586, wildcard= Deadline because the BUDGET is too low. Relaxing price filter...\n",
      "Warning: No translators found for task 11180447, wildcard= Price because the QUALITY is too high. Relaxing quality filter...\n",
      "Warning: No translators found for task 10385846, wildcard= Deadline because the BUDGET is too low. Relaxing price filter...\n",
      "Top-5 Accuracy: 20.00%\n"
     ]
    }
   ],
   "source": [
    "# ----- MAIN CODE ----\n",
    "# Creates a dataframe with  the additional attributes\n",
    "translators_attributes_df = compute_delay_percentage(data_df, transl_cost_pairs_df)\n",
    "translators_attributes_df = compute_number_tasks(data_df, translators_attributes_df) \n",
    "\n",
    "validation_df_clean = validation_df_clean[0:100]  # For testing purposes, limit to 100 rows\n",
    "# Define the top-k value for evaluation\n",
    "k = 5\n",
    "correct_predictions = 0\n",
    "total_predictions = 0\n",
    "\n",
    "# Iterate over each row in the validation set\n",
    "for idx, task_row in validation_df_clean.iterrows():\n",
    "    #Convert the current row to a task\n",
    "    new_task = task_row.copy()\n",
    "    new_task = new_task.rename({'HOURLY_RATE': 'SELLING_HOURLY_PRICE', 'QUALITY_EVALUATION': 'MIN_QUALITY'})\n",
    "    \n",
    "    match = clients_df[clients_df['CLIENT_NAME'].str.strip() == new_task['MANUFACTURER'].strip()]\n",
    "\n",
    "    if not match.empty:\n",
    "        new_task['WILDCARD'] = match.iloc[0]['WILDCARD']\n",
    "        new_task['HOURLY_RATE'] = match.iloc[0]['SELLING_HOURLY_PRICE']\n",
    "        new_task['QUALITY_EVALUATION'] = match.iloc[0]['MIN_QUALITY']\n",
    "    else:\n",
    "        print(\"WARNING: No match found in schedules_df for the given client. Setting default values.\")\n",
    "        new_task['WILDCARD'] = 'Quality'\n",
    "        new_task['HOURLY_RATE'] = new_task['SELLING_HOURLY_PRICE']\n",
    "        new_task['QUALITY_EVALUATION'] = new_task['MIN_QUALITY']\n",
    "    \n",
    "    new_task = Task(**new_task.to_dict())  # Convert the task to the Task object\n",
    "    \n",
    "    # Filter translators based on task attributes\n",
    "    df_filtered = filter_language_price_quality_availability(data_df, schedules_df, translators_attributes_df, new_task)\n",
    "\n",
    "    # Compute experience scores for the filtered translators\n",
    "    compute_experience(df_filtered, data_df, task_type=new_task.TASK_TYPE, source_lang=new_task.SOURCE_LANG, target_lang=new_task.TARGET_LANG, industry=new_task.MANUFACTURER_INDUSTRY, subindustry=new_task.MANUFACTURER_SUBINDUSTRY)\n",
    "    compute_experience_for_client(df_filtered, data_df, client=new_task.MANUFACTURER)\n",
    "\n",
    "    # Get the distances and indexes from KNN\n",
    "    distances, indexes = knn(df_filtered, data_df, new_task, metric='euclidean', need_wildcard=False)\n",
    "    \n",
    "    # Get the best translators\n",
    "    selected_translators = get_best_translators(df_filtered, indexes, distances)\n",
    "    \n",
    "    # Retrieve the true translator label for the current task\n",
    "    true_translator = validation_translator_labels[idx]  # Assuming you have the true labels in the dictionary from earlier\n",
    "    \n",
    "    # Check if any of the top-k translators match the true translator\n",
    "    top_k_translators = selected_translators.iloc[:k]  # Top-k translators\n",
    "    if true_translator in top_k_translators['TRANSLATOR'].values:\n",
    "        correct_predictions += 1\n",
    "    \n",
    "    total_predictions += 1\n",
    "\n",
    "# Calculate the top-k accuracy\n",
    "top_k_accuracy = correct_predictions / total_predictions\n",
    "print(f\"Top-{k} Accuracy: {top_k_accuracy * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
